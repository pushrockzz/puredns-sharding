name: puredns-quick-test

on:
  workflow_dispatch:
    inputs:
      chunk_start:
        description: 'First chunk index this run will process (integer)'
        required: true
        default: '0'
      total_chunks:
        description: 'DEPRECATED: Total chunks is now auto-detected.'
        required: false
      run_counter:
        description: 'Automatic run counter for recursion (auto-incremented)'
        required: false
        default: '0'
      max_runs:
        description: 'Safety cap: stop after this many runs. If unset, it is auto-calculated.'
        required: false
        default: '999'
      words_per_chunk:
        description: 'Words per chunk file (lines_per_chunk).'
        required: false
        default: '310000'
      domains_count:
        description: 'How many test domains to generate (default = 5)'
        required: false
        default: '2'

env:
  WORKFLOW_FILE: puredns-quicktest.yml
  MATRIX_SIZE: '20'
  COMBINED_WORDLIST: combined-wordlist.txt
  REMOTE_ZIP_URL: https://localdomain.pw/subdomain-bruteforce-list/all.txt.zip
  CHUNK_LINES: "310000"
  SORT_MEM: "4G"  

jobs:
  prepare:
    name: prepare-wordlists-and-resolvers
    runs-on: ubuntu-latest
    env:
      COMBINED_WORDLIST: combined-wordlist.txt
    outputs:
      artifact-name: wordlists-artifact
      total-chunks: ${{ steps.finalize_prep.outputs.total-chunks }}
      effective-max-runs: ${{ steps.finalize_prep.outputs.effective-max-runs }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0   
          
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23' 

      - name: Cache Raw Wordlists (restore)
        uses: actions/cache@v3
        id: cache-raw
        with:
          path: |
            best-wordlist-level1.txt
            best-wordlist-level2.txt
          key: raw-wordlists-cache-v1-${{ runner.os }}-${{ hashFiles('best-wordlist-*.txt') }}

      - name: Fetch default small wordlists & resolvers (if missing)
        run: |
          
          if [ ! -f best-wordlist-level1.txt ]; then wget -qO best-wordlist-level1.txt https://raw.githubusercontent.com/trickest/wordlists/main/inventory/levels/level1.txt; fi
          if [ ! -f best-wordlist-level2.txt ]; then wget -qO best-wordlist-level2.txt https://raw.githubusercontent.com/trickest/wordlists/main/inventory/levels/level2.txt; fi
          wget -qO resolvers.txt https://raw.githubusercontent.com/rix4uni/resolvers/main/resolvers.txt || true
          wget -qO resolvers-trusted.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/main/resolvers.txt || true

      - name: Download big remote wordlists (the ones you listed)
        run: |
          
          mkdir -p wordlists
          cd wordlists
          # list of URLs (one per line)
          urls=(
            "https://media.githubusercontent.com/media/Pcoder7/All-In-One-DNS-Wordlist/refs/heads/main/2m-subdomains-2167059.txt"
            "https://media.githubusercontent.com/media/Pcoder7/All-In-One-DNS-Wordlist/refs/heads/main/best-wordlist-9996122.txt"
            "https://media.githubusercontent.com/media/Pcoder7/All-In-One-DNS-Wordlist/refs/heads/main/seclistes-dns-7110368.txt"
            "https://media.githubusercontent.com/media/Pcoder7/All-In-One-DNS-Wordlist/refs/heads/main/http-subdomains-2441323.txt"
            "https://media.githubusercontent.com/media/Pcoder7/All-In-One-DNS-Wordlist/refs/heads/main/mily-880196.txt"
            "https://media.githubusercontent.com/media/Pcoder7/All-In-One-DNS-Wordlist/refs/heads/main/ass-420113.txt"
            "https://media.githubusercontent.com/media/Pcoder7/All-In-One-DNS-Wordlist/refs/heads/main/danr-649649.txt"
            "https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/refs/heads/main/n0kovo_subdomains_huge.txt"
            "https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/refs/heads/main/n0kovo_subdomains_large.txt"
            "https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/refs/heads/main/n0kovo_subdomains_medium.txt"
            "https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/refs/heads/main/n0kovo_subdomains_small.txt"
            "https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/refs/heads/main/n0kovo_subdomains_tiny.txt"
          )
          for u in "${urls[@]}"; do
            fname=$(basename "$u")
            # skip if already downloaded
            if [ -s "$fname" ]; then
              echo "exists: $fname"
              continue
            fi
            echo "Downloading $u -> $fname"
            # use -c to resume; use -q for quiet; fail the step if download fails
            wget -c -q -O "$fname" "$u"
            if [ ! -s "$fname" ]; then
              echo "Warning: $fname empty after download"
            else
              echo "Downloaded $fname ($(wc -l < $fname || true) lines)"
            fi
          done
          cd ..

      - name: Download remote all.txt.zip and extract all.txt
        run: |
          
          echo "Downloading zip via wget..."
          wget -qO all.txt.zip "${{ env.REMOTE_ZIP_URL }}"
          echo "Unzipping to all.txt ..."
          unzip -p all.txt.zip > all.txt
          echo "all.txt lines: $(wc -l < all.txt || true)"

      - name: Build filter_wordlist tool (if present)
        id: build-filter
        run: |
          
          if [ -f "filter_wordlist.go" ]; then
            echo "Building filter_wordlist..."
            go build -o filter_wordlist filter_wordlist.go
            echo "built=1" >> $GITHUB_OUTPUT
          else
            echo "built=0" >> $GITHUB_OUTPUT
          fi

      - name: Try restore cached filtered wordlist
        id: restore-filtered
        uses: actions/cache@v3
        with:
          key: filtered-wordlist-cache-v1-${{ runner.os }}-${{ hashFiles('best-wordlist-*.txt','filter_wordlist.go') }}
          path: |
            best-wordlist-filtered.txt
            best-wordlist-level2-filtered.txt
            all-filtered.txt
            ${{ env.COMBINED_WORDLIST }}

      - name: Filter large wordlists using filter_wordlist (chunked + parallel)
        if: steps.restore-filtered.outputs.cache-hit != 'true'
        run: |
          
          # config and defaults
          FILTER_CHUNK_LINES=${FILTER_CHUNK_LINES:-500000}
          DEDUPE_CHUNK_LINES=${DEDUPE_CHUNK_LINES:-1000000}
          WORKDIR=$(pwd)
          mkdir -p work_filtered
          rm -rf work_filtered/* || true

          # helper function: filter a single file (chunked if large)
          filter_file() {
            local src="$1"
            local base
            base=$(basename "$src")
            local out="$WORKDIR/work_filtered/${base}.filtered"
            echo "----- processing $src -> $out -----"
            # ensure file exists and is non-empty
            if [ ! -s "$src" ]; then
              echo "Skipping empty/missing $src"
              return 0
            fi
            local lines
            lines=$(wc -l < "$src" || echo 0)
            if [ "$lines" -le "$FILTER_CHUNK_LINES" ]; then
              # small enough: filter in one shot if tool exists else normalize
              if [ -x ./filter_wordlist ]; then
                ./filter_wordlist "$src" > "$out" || true
              else
                tr '[:upper:]' '[:lower:]' < "$src" | sed 's/[^[:print:]]//g' | awk 'NF' > "$out" || true
              fi
              echo "Produced $out ($(wc -l < "$out" || true) lines)"
              return 0
            fi

            # large file: split into chunks and filter chunks in parallel
            local chdir="$WORKDIR/work_chunks_${base%.*}"
            rm -rf "$chdir"
            mkdir -p "$chdir"
            echo "Splitting $src into chunks of $FILTER_CHUNK_LINES lines into $chdir..."
            split -l "$FILTER_CHUNK_LINES" --numeric-suffixes=1 --additional-suffix=.chunk "$src" "$chdir/chunk_"

            # process chunks in parallel (throttled to nproc)
            nproc=$(nproc || echo 2)
            printf '%s\0' "$chdir"/chunk_* | xargs -0 -n1 -P "$nproc" -I{} bash -c '\
              chunk="{}"; \
              outchunk="${chunk}.filtered"; \
              if [ -x ./filter_wordlist ]; then \
                ./filter_wordlist "$chunk" > "$outchunk" || true; \
              else \
                tr "[:upper:]" "[:lower:]" < "$chunk" | sed "s/[^[:print:]]//g" | awk "NF" > "$outchunk" || true; \
              fi; \
              echo "filtered $chunk -> $outchunk"'

            # concatenate filtered chunks into final filtered file
            cat "$chdir"/chunk_*.filtered > "$out"
            echo "Concatenated filtered chunks -> $out ($(wc -l < "$out" || true) lines)"
            # optional: remove chunk dir to save disk
            rm -rf "$chdir"
          }

          # Build list of source files:
          # local small pre-filters
          sources=()
          # include local level lists if present
          [ -f best-wordlist-level1.txt ] && sources+=("best-wordlist-level1.txt")
          [ -f best-wordlist-level2.txt ] && sources+=("best-wordlist-level2.txt")
          # include all filtered from repo downloads
          for f in wordlists/*; do
            [ -f "$f" ] || continue
            sources+=("$f")
          done
          # include the all.txt you unzipped
          [ -f all.txt ] && sources+=("all.txt")

          echo "Files to filter: ${#sources[@]}"
          for s in "${sources[@]}"; do
            echo "-> $s"
          done

          # loop and filter each file (serial loop, individual filtering uses parallel chunks)
          for s in "${sources[@]}"; do
            filter_file "$s"
          done

          # combine all filtered files into a single all-filtered.txt for dedupe
          rm -f all-filtered.txt || true
          for f in work_filtered/*.filtered; do
            [ -f "$f" ] || continue
            cat "$f" >> all-filtered.txt
          done
          echo "Combined all-filtered.txt lines: $(wc -l < all-filtered.txt || true)"
          ls -lh all-filtered.txt || true

      - name: Chunk, sort and produce merged sorted file (dedupe across all-filtered.txt)
        if: steps.restore-filtered.outputs.cache-hit != 'true'
        run: |
          
          IN_FILE=all-filtered.txt
          CHUNK_DIR=dedupe_chunks
          MERGED_SORTED=all-merged.sorted.txt
          TMPDIR=${TMPDIR:-/tmp}
          SORT_MEM=${SORT_MEM:-1G}
          DEDUPE_CHUNK_LINES=${DEDUPE_CHUNK_LINES:-1000000}

          if [ -z "${DEDUPE_CHUNK_LINES:-}" ] || ! printf '%s' "${DEDUPE_CHUNK_LINES}" | grep -Eq '^[0-9]+$'; then
            echo "DEDUPE_CHUNK_LINES invalid; defaulting to 1000000"; DEDUPE_CHUNK_LINES=1000000
          fi

          if [ ! -f "$IN_FILE" ]; then
            echo "ERROR: $IN_FILE missing"; exit 1
          fi

          echo "Input total lines: $(wc -l < "$IN_FILE" || true)"
          rm -rf "$CHUNK_DIR"
          mkdir -p "$CHUNK_DIR"

          echo "Splitting into chunks of $DEDUPE_CHUNK_LINES lines..."
          split -l "${DEDUPE_CHUNK_LINES}" --numeric-suffixes=1 --additional-suffix=.chunk "$IN_FILE" "$CHUNK_DIR/chunk_"

          chunk_count=$(ls "$CHUNK_DIR"/chunk_*.chunk 2>/dev/null | wc -l || true)
          if [ -z "$chunk_count" ] || [ "$chunk_count" -eq 0 ]; then
            echo "No chunk files produced; falling back to single sort -u"
            LC_ALL=C sort -u -T "$TMPDIR" -S "$SORT_MEM" "$IN_FILE" -o "$MERGED_SORTED"
            echo "Merged sorted file produced: $MERGED_SORTED ($(wc -l < $MERGED_SORTED || true) lines)"
            exit 0
          fi

          echo "Sorting each dedupe chunk (parallel up to nproc)..."
          nproc=$(nproc || echo 2)
          jobs=0
          for f in "$CHUNK_DIR"/chunk_*.chunk; do
            out="${f}.sorted"
            ( LC_ALL=C sort -u -T "$TMPDIR" -S "$SORT_MEM" "$f" -o "$out" && echo "sorted -> $out" ) &
            jobs=$((jobs+1))
            if [ $jobs -ge $nproc ]; then
              wait -n || true
              jobs=$((jobs-1))
            fi
          done
          wait

          sorted_count=$(ls "$CHUNK_DIR"/chunk_*.chunk.sorted 2>/dev/null | wc -l || true)
          if [ -z "$sorted_count" ] || [ "$sorted_count" -eq 0 ]; then
            echo "ERROR: no sorted chunk files found after sorting"; exit 1
          fi

          echo "Merging sorted chunks via repo-root Python script..."
          python3 merge_sorted_chunks.py "$CHUNK_DIR" "$MERGED_SORTED"
          echo "Merged sorted file produced: $MERGED_SORTED ($(wc -l < $MERGED_SORTED || true) lines)"
          ls -lh "$MERGED_SORTED" || true

      - name: Ensure anew is available (install if missing)
        if: steps.restore-filtered.outputs.cache-hit != 'true'
        run: |
          
          if ! command -v anew >/dev/null 2>&1; then
            echo "anew not found; installing via go install ..."
            export GOBIN="$HOME/go/bin"
            mkdir -p "$GOBIN"
            PATH="$GOBIN:$PATH"
            go install github.com/tomnomnom/anew@latest
          else
            echo "anew already present"
          fi
          echo "anew: $(command -v anew || true)"

      - name: Combine using anew (incremental, memory-friendly)
        if: steps.restore-filtered.outputs.cache-hit != 'true'
        run: |
          
          PATH="$HOME/go/bin:$PATH"
          MERGED_SORTED=all-merged.sorted.txt
          rm -f "${{ env.COMBINED_WORDLIST }}" || true
          touch "${{ env.COMBINED_WORDLIST }}"

          # small lists first
          if [ -f best-wordlist-filtered.txt ]; then
            cat best-wordlist-filtered.txt | anew -q "${{ env.COMBINED_WORDLIST }}" || true
          fi
          if [ -f best-wordlist-level2-filtered.txt ]; then
            cat best-wordlist-level2-filtered.txt | anew -q "${{ env.COMBINED_WORDLIST }}" || true
          fi

          # big merged sorted: feed in parts
          if [ -f "$MERGED_SORTED" ]; then
            rm -rf merged_parts || true
            mkdir -p merged_parts
            split -l 500000 --numeric-suffixes=1 --additional-suffix=.part "$MERGED_SORTED" merged_parts/part_
            for p in merged_parts/part_*; do
              cat "$p" | anew -q "${{ env.COMBINED_WORDLIST }}" || true
            done
          fi

          echo "Final COMBINED_WORDLIST lines: $(wc -l < ${COMBINED_WORDLIST} || true)"
          ls -lh "${COMBINED_WORDLIST}" || true

      - name: Save filtered wordlist to cache (store)
        if: steps.restore-filtered.outputs.cache-hit != 'true'
        uses: actions/cache@v3
        with:
          key: filtered-wordlist-cache-v1-${{ runner.os }}-${{ hashFiles('best-wordlist-*.txt','filter_wordlist.go') }}
          path: |
            best-wordlist-filtered.txt
            best-wordlist-level2-filtered.txt
            all-filtered.txt
            all-merged.sorted.txt
            "${{ env.COMBINED_WORDLIST }}"

      - name: Final check (always)
        run: |
          echo "Combined wordlist present:"
          ls -lh "${{ env.COMBINED_WORDLIST }}" || true
          head -n 10 "${{ env.COMBINED_WORDLIST }}" || true
          
      - name: Finalize Preparation and Split Chunks
        id: finalize_prep
        run: |
          mkdir -p domains wordlists results
          if [ ! -f "domains/domains.txt" ]; then
            for i in $(seq 1 "${{ github.event.inputs.domains_count }}"); do echo "test${i}.example.com"; done > "domains/domains.txt"
          fi
          COMBINED_WORDLIST="${{ env.COMBINED_WORDLIST }}"
          if [ ! -s "$COMBINED_WORDLIST" ]; then echo "::error:: Combined wordlist is empty."; exit 0; fi
          rm -f wordlists/chunk-*.txt || true
          split -l "${{ github.event.inputs.words_per_chunk }}" -d -a 5 --additional-suffix=.txt "$COMBINED_WORDLIST" wordlists/chunk-
          if [ -z "$(ls -A wordlists)" ]; then echo "::error:: Split failed to create chunks."; exit 0; fi
          CHUNK_COUNT=$(ls -1 wordlists/chunk-*.txt | wc -l)
          echo "total-chunks=${CHUNK_COUNT}" >> $GITHUB_OUTPUT
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          CALCULATED_RUNS=$(( (CHUNK_COUNT + MATRIX_SIZE - 1) / MATRIX_SIZE ))
          USER_MAX_RUNS=${{ github.event.inputs.max_runs }}
          EFFECTIVE_MAX_RUNS=$(( CALCULATED_RUNS < USER_MAX_RUNS ? CALCULATED_RUNS : USER_MAX_RUNS ))
          echo "effective-max-runs=${EFFECTIVE_MAX_RUNS}" >> $GITHUB_OUTPUT
      
      - name: Upload binaries and wordlists as artifact
        uses: actions/upload-artifact@v4
        with:
          name: wordlists-artifact
          path: |
            wordlists
            resolvers.txt
            resolvers-trusted.txt
            domains/domains.txt
          retention-days: 1     
 
  coordinate_and_dispatch:
    name: Coordinate & Dispatch Batches
    runs-on: ubuntu-latest
    needs: prepare
    env:
      TOTAL_CHUNKS: ${{ needs.prepare.outputs.total-chunks }}

    outputs:
      primary_shard_matrix: ${{ steps.gen_primary_matrix.outputs.shard_matrix }}
      next_recursive_start: ${{ steps.calculate_indices.outputs.next_start }}
    steps:
      - name: Calculate Batch Indices
        id: calculate_indices
        shell: bash
        run: |
          PRIMARY_START=${{ github.event.inputs.chunk_start }}
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          
          SECONDARY_START=$((PRIMARY_START + MATRIX_SIZE))
          NEXT_RECURSIVE_START=$((PRIMARY_START + 2 * MATRIX_SIZE))
          
          echo "Primary batch starts at: ${PRIMARY_START}"
          echo "Secondary batch starts at: ${SECONDARY_START}"
          echo "Next recursive run will start at: ${NEXT_RECURSIVE_START}"
          
          echo "secondary_start=${SECONDARY_START}" >> $GITHUB_OUTPUT
          echo "next_start=${NEXT_RECURSIVE_START}" >> $GITHUB_OUTPUT

      - name: Generate Matrix for Primary Workflow (This Run)
        id: gen_primary_matrix
        shell: bash
        run: |
          TOTAL_CHUNKS=${{ env.TOTAL_CHUNKS }}
          CHUNK_START=${{ github.event.inputs.chunk_start }}
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          
          CHUNKS_REMAINING=$(( TOTAL_CHUNKS - CHUNK_START ))
          if (( CHUNKS_REMAINING < 0 )); then CHUNKS_REMAINING=0; fi
          
          SHARDS_THIS_RUN=$(( CHUNKS_REMAINING < MATRIX_SIZE ? CHUNKS_REMAINING : MATRIX_SIZE ))
          
          if (( SHARDS_THIS_RUN <= 0 )); then
            echo "shard_matrix=[]" >> $GITHUB_OUTPUT
          else
            MATRIX_JSON=$(jq -cn --argjson n "$SHARDS_THIS_RUN" '[range($n)]')
            echo "Generated primary matrix for ${SHARDS_THIS_RUN} shards: $MATRIX_JSON"
            echo "shard_matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT
          fi

      # <-- NEW STEP: Perform a reliable numeric comparison in Bash -->
      - name: Check if Secondary Worker Should Run
        id: should_trigger_secondary
        shell: bash
        run: |
          # Use bash's numeric comparison operator `lt` (less than)
          if [[ "${{ steps.calculate_indices.outputs.secondary_start }}" -lt "${{ env.TOTAL_CHUNKS }}" ]]; then
            echo "Decision: Secondary worker HAS work to do. (20 < 122)"
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "Decision: Secondary worker has NO work to do."
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi

      - name: Trigger Secondary Worker Workflow
        # <-- MODIFIED LINE: Use the reliable boolean output from the previous step -->
        if: steps.should_trigger_secondary.outputs.should_run == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: puredns-quicktest.yml
          repo: ${{ secrets.SECONDARY_REPO_OWNER }}/${{ secrets.SECONDARY_REPO_NAME }}
          token: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          ref: main
          inputs: >
            {
              "primary_run_id": "${{ github.run_id }}",
              "primary_repo_owner": "${{ github.repository_owner }}",
              "primary_repo_name": "${{ github.event.repository.name }}",
              "artifact_name": "${{ needs.prepare.outputs.artifact-name }}",
              "chunk_start": "${{ steps.calculate_indices.outputs.secondary_start }}",
              "run_counter": "${{ github.event.inputs.run_counter }}",
              "max_runs": "1",
              "words_per_chunk": "${{ github.event.inputs.words_per_chunk }}",
              "domains_count": "${{ github.event.inputs.domains_count }}"
            }
  brute:
    name: bruteforce-shard (shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    needs: [prepare, coordinate_and_dispatch]
    if: needs.coordinate_and_dispatch.outputs.primary_shard_matrix != '[]'
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      fail-fast: false
      matrix:
        shard: ${{ fromJson(needs.coordinate_and_dispatch.outputs.primary_shard_matrix) }}
    steps:
      - name: Checkout repo (for committing results)
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0   
          
      - name: Download artifacts (binaries and wordlists)
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare.outputs.artifact-name }}
          path: .

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-
            
      - name: Install Tools
        run: |
          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi    
          
      - name: Run puredns bruteforce
        shell: bash
        run: |
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          CHUNK_FILE="wordlists/chunk-$(printf '%05d' "$CHUNK_INDEX").txt"
          if [ ! -f "$CHUNK_FILE" ]; then exit 0; fi
          while IFS= read -r domain || [ -n "$domain" ]; do
            case "$domain" in \#*|"") continue ;; esac
            ROOT_DOMAIN_DIR="results/$domain"
            mkdir -p "$ROOT_DOMAIN_DIR"
            cat "$CHUNK_FILE" | puredns bruteforce "$domain" -r "resolvers.txt" --rate-limit 5000 --rate-limit-trusted 2000 \
              --resolvers-trusted "resolvers-trusted.txt" --wildcard-tests 300 --wildcard-batch 100000 \
              --write "${ROOT_DOMAIN_DIR}/puredns-output-${CHUNK_INDEX}.txt" \
              --write-wildcards "${ROOT_DOMAIN_DIR}/wildcards-${CHUNK_INDEX}.txt" \
              --write-massdns "${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt" --quiet
            PUREDNS_FILE="${ROOT_DOMAIN_DIR}/puredns-output-${CHUNK_INDEX}.txt"
            MASSDNS_FILE="${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt"
          
            # per-domain tmpdir for isolation
            SAFE_DOMAIN_NAME=$(echo "$domain" | sed 's/[^A-Za-z0-9._-]/_/g')
            DOMAIN_TMPDIR=$(mktemp -d -p "$(pwd)" "tmp-${SAFE_DOMAIN_NAME}-XXXX")
            TMP_PURE="$DOMAIN_TMPDIR/puredns-valid.txt"
            TMP_CLEAN="$DOMAIN_TMPDIR/massdns-clean.txt"
            TMP_FILTER="$DOMAIN_TMPDIR/massdns-filtered.txt"
          
            # Build normalized PureDNS host list (strip trailing dot, lowercase, unique) if present
            if [ -s "$PUREDNS_FILE" ]; then
              awk '{ gsub(/\.$/,"",$1); print tolower($1) }' "$PUREDNS_FILE" | sort -u > "$TMP_PURE"
              PURE_COUNT=$(wc -l < "$TMP_PURE" 2>/dev/null || echo 0)
            else
              PURE_COUNT=0
              : > "$TMP_PURE"
            fi
          
            # If massdns exists, clean it into canonical lines "host A ip"
            if [ -s "$MASSDNS_FILE" ]; then
              awk 'NF {
                # normalize host (first field), find first "A" token (any field) and take next field as IP
                host=$1; sub(/\.$/,"",host);
                aidx=0
                for(i=2;i<=NF;i++){
                  if($i=="A"){ aidx=i; break }
                }
                if(aidx>0 && (aidx+1)<=NF){
                  ip=$(aidx+1);
                  gsub(/^[ \t]+|[ \t]+$/,"",host);
                  gsub(/^[ \t]+|[ \t]+$/,"",ip);
                  if(host!="" && ip!="") print host " A " ip;
                }
              }' "$MASSDNS_FILE" | sort -u > "$TMP_CLEAN" || true
            else
              # no massdns produced (create empty clean file)
              : > "$TMP_CLEAN"
            fi
          
            # Filter cleaned massdns against puredns hostlist if puredns exists; otherwise keep cleaned entries
            if [ -s "$TMP_PURE" ]; then
              awk -v validfile="$TMP_PURE" '
                BEGIN {
                  while((getline v < validfile) > 0) valid[tolower(v)] = 1
                  close(validfile)
                }
                NF>=3 {
                  host=$1; host_l=tolower(host);
                  if(host_l in valid) print $0
                }
              ' "$TMP_CLEAN" > "$TMP_FILTER" || true
            else
              cp "$TMP_CLEAN" "$TMP_FILTER" || true
            fi
          
            # Atomic replace: move filtered output back to domain massdns file
            mv "$TMP_FILTER" "$MASSDNS_FILE"
            rm -f "$TMP_CLEAN"
          
            # Count massdns lines for echo (safe even if empty)
            MASSDNS_COUNT=$(wc -l < "$MASSDNS_FILE" 2>/dev/null || echo 0)
          
            # Echo exact messages you requested
            echo "✅ PureDNS resolution complete. ${PURE_COUNT} subdomains were successfully resolved."
            echo "✅ PureDNS resolution complete. ${MASSDNS_COUNT} massdns found."
          
            # remove per-domain tmpdir
            rm -rf "$DOMAIN_TMPDIR"
          
          done < "domains/domains.txt"
          
      - name: Perform Port Scanning
        shell: bash
        run: |
          
          
          PORTS=$(tr -d '\r' < ports.txt | tr -d '[:space:]')
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          
          # Create a single, global temporary directory for this entire step's operations
          GLOBAL_TMPDIR=$(mktemp -d)
          # This 'trap' command ensures the temporary directory is automatically deleted when the step exits
          trap 'rm -rf "$GLOBAL_TMPDIR"' EXIT
          # --- PHASE 1: AGGREGATE ALL TARGETS ---
          echo "PHASE 1: Aggregating all targets from all domains..."
          
          # Define paths for our new master files inside the safe temporary directory
          MASTER_IP_TO_SUBDOMAIN_MAP="${GLOBAL_TMPDIR}/master_ip_to_subdomain.txt"
          MASTER_UNIQUE_IPS="${GLOBAL_TMPDIR}/master_unique_ips.txt"
          # Initialize empty files
          : > "$MASTER_IP_TO_SUBDOMAIN_MAP"
          # Loop through domains for the sole purpose of collecting IPs and mapping them.
          while IFS= read -r domain || [ -n "$domain" ]; do
              case "$domain" in \#*|"") continue ;; esac
              ROOT_DOMAIN_DIR="results/$domain"
              MASSDNS_INPUT_FILE="${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt"
              if [ ! -s "$MASSDNS_INPUT_FILE" ]; then
                  continue
              fi
              # Read the massdns file, clean it, and create a map of "IP SUBDOMAIN".
              # Append this to our master map file. This preserves the crucial link.
              awk 'NF>=3 && $2=="A" { host=$1; sub(/\.$/,"",host); print $3, host }' "$MASSDNS_INPUT_FILE" \
                | sort -u >> "$MASTER_IP_TO_SUBDOMAIN_MAP" || true
          done < "domains/domains.txt"
          if [ ! -s "$MASTER_IP_TO_SUBDOMAIN_MAP" ]; then
              echo "⚠️  No IPs found across any domains. Exiting port scanning step."
              exit 0
          fi
          # Create the master list of unique IPs to be scanned from our map.
          cut -d' ' -f1 "$MASTER_IP_TO_SUBDOMAIN_MAP" | sort -u > "$MASTER_UNIQUE_IPS" || true
          echo "✅ Phase 1 Complete: Collected $(wc -l < "$MASTER_UNIQUE_IPS" 2>/dev/null || echo 0) unique IPs from $(wc -l < "$MASTER_IP_TO_SUBDOMAIN_MAP" 2>/dev/null || echo 0) total mappings."
          # --- PHASE 2: PERFORM A SINGLE, CONSOLIDATED SCAN ---
          echo "PHASE 2: Performing one consolidated scan..."
          MASTER_NON_CDN_IPS="${GLOBAL_TMPDIR}/master_non_cdn.txt"
          MASTER_SCAN_results="${GLOBAL_TMPDIR}/master_scan_results.txt"
          
          # Initialize empty file
          : > "$MASTER_NON_CDN_IPS"
          
          # Run CDN check ONCE on the master IP list
          cat "$MASTER_UNIQUE_IPS" | cut-cdn -ua -t 50 -silent -o "$MASTER_NON_CDN_IPS" || true
          # If any non-CDN IPs exist, run the scanners ONCE
          if [ -s "$MASTER_NON_CDN_IPS" ]; then
              echo "  -> Scanning $(wc -l < "$MASTER_NON_CDN_IPS" 2>/dev/null || echo 0) non-CDN IPs..."
              TMP_NAABU_RUSTSCAN=$(mktemp)
              TMP_RUSTSCAN=$(mktemp)
              
              # Your original tool commands, unchanged
              
              naabu -l "$MASTER_NON_CDN_IPS" -passive -o "$TMP_NAABU_RUSTSCAN" -no-color -silent || true
              
              rustscan -a "$MASTER_NON_CDN_IPS" -p "$PORTS" --no-banner -t 1000 --tries 1 -u 50000 --scan-order "Random" -b 500 --greppable --accessible > "$TMP_RUSTSCAN" || true
              
              # Your original processing logic, unchanged
              cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }' | anew -q "$TMP_NAABU_RUSTSCAN" || true
              
              # Process scan results into consistent format: "IP PORT"
              awk -F: 'NF==2 { print $1, $2 } NF==1 { print $1, "NOPORT" }' "$TMP_NAABU_RUSTSCAN" | sort -u > "$MASTER_SCAN_results" || true
              rm -f "$TMP_RUSTSCAN" "$TMP_NAABU_RUSTSCAN" 2>/dev/null || true
          else
              # Create empty scan results file
              : > "$MASTER_SCAN_results"
          fi
          
          # Add CDN IPs back to the results list with NOPORT marker for consistency
          MASTER_CDN_IPS="${GLOBAL_TMPDIR}/master_cdn_ips.txt"
          comm -13 <(sort "$MASTER_NON_CDN_IPS" 2>/dev/null || true) <(sort "$MASTER_UNIQUE_IPS" 2>/dev/null || true) > "$MASTER_CDN_IPS" 2>/dev/null || true
          if [ -s "$MASTER_CDN_IPS" ]; then
              awk '{ print $1, "NOPORT" }' "$MASTER_CDN_IPS" >> "$MASTER_SCAN_results" || true
          fi
          
          echo "✅ Phase 2 Complete: Scan finished ($(wc -l < "$MASTER_SCAN_results" 2>/dev/null || echo 0) total IP-port combinations)."
          # --- PHASE 3: DISTRIBUTE results ---
          echo "PHASE 3: Distributing results to domain directories..."
          
          # Sort master files once for efficient processing
          MASTER_SCAN_results_SORTED="${GLOBAL_TMPDIR}/master_scan_sorted.txt"
          MASTER_MAP_SORTED="${GLOBAL_TMPDIR}/master_map_sorted.txt"
          sort -k1,1 "$MASTER_SCAN_results" > "$MASTER_SCAN_results_SORTED" 2>/dev/null || true
          sort -k1,1 "$MASTER_IP_TO_SUBDOMAIN_MAP" > "$MASTER_MAP_SORTED" 2>/dev/null || true
          
          # Verify sorted files exist
          if [ ! -f "$MASTER_SCAN_results_SORTED" ] || [ ! -f "$MASTER_MAP_SORTED" ]; then
              echo "⚠️  Error: Failed to create sorted master files. Exiting Phase 3."
              exit 0
          fi
          
          # Loop through domains a final time to generate their specific report files
          DOMAINS_PROCESSED=0
          DOMAINS_SKIPPED=0
          
          while IFS= read -r domain || [ -n "$domain" ]; do
              case "$domain" in \#*|"") continue ;; esac
              ROOT_DOMAIN_DIR="results/$domain"
              OUTPUT_FILE="${ROOT_DOMAIN_DIR}/subdomain_ports-${CHUNK_INDEX}.txt"
              
              # Escape domain for safe regex matching - only escape dots for regex
              ESCAPED_DOMAIN=$(printf '%s\n' "$domain" | sed 's/\./\\./g' 2>/dev/null || echo "$domain")
              
              # Extract only the map entries for this specific domain
              # Pattern: matches any line where second field ends with .domain.com OR is exactly domain.com
              # The map format is: "IP SUBDOMAIN", so we match on the SUBDOMAIN field
              DOMAIN_MAP_TMP=$(mktemp)
              awk -v domain="$domain" '
                $2 == domain || $2 ~ "\\." domain "$" { print $0 }
              ' "$MASTER_MAP_SORTED" > "$DOMAIN_MAP_TMP" 2>/dev/null || true
              if [ ! -s "$DOMAIN_MAP_TMP" ]; then
                  echo "  -> ⚠️  Skipping $domain (no IPs found in master map)"
                  DOMAINS_SKIPPED=$((DOMAINS_SKIPPED + 1)) || true
                  rm -f "$DOMAIN_MAP_TMP" 2>/dev/null || true
                  continue
              fi
              
              # Join scan results with this domain's IP-subdomain map
              # Output format after join: IP PORT SUBDOMAIN
              join -t' ' -1 1 -2 1 "$MASTER_SCAN_results_SORTED" "$DOMAIN_MAP_TMP" 2>/dev/null | \
                awk '{
                    ip = $1
                    port = $2
                    subdomain = $3
                    
                    # If port exists and is not NOPORT marker, format as subdomain:port
                    if (port != "NOPORT" && port ~ /^[0-9]+$/) {
                        print subdomain ":" port
                    } else {
                        # No port data (CDN IP), just print subdomain
                        print subdomain
                    }
                  }' 2>/dev/null | \
                sort -u > "$OUTPUT_FILE" 2>/dev/null || true
              if [ -s "$OUTPUT_FILE" ]; then
                  LINE_COUNT=$(wc -l < "$OUTPUT_FILE" 2>/dev/null || echo 0)
                  echo "  -> ✅ Generated $OUTPUT_FILE for $domain ($LINE_COUNT entries)"
                  DOMAINS_PROCESSED=$((DOMAINS_PROCESSED + 1)) || true
              else
                  echo "  -> ⚠️  Warning: $domain has no port scan results (empty output)"
                  DOMAINS_SKIPPED=$((DOMAINS_SKIPPED + 1)) || true
              fi
              
              rm -f "$DOMAIN_MAP_TMP" 2>/dev/null || true
          done < "domains/domains.txt"
          
          echo "✅ Phase 3 Complete: $DOMAINS_PROCESSED domains processed, $DOMAINS_SKIPPED domains skipped."
          
          # Explicitly exit with success
          exit 0
          
      - name: Sanitize and Finalize results
        shell: bash
        run: |
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          echo "DEBUG: matrix.shard='${{ matrix.shard }}' chunk_start='${{ github.event.inputs.chunk_start }}' CHUNK_INDEX=$CHUNK_INDEX"
          
          # show any pre-existing result files for this chunk (if any)
          find "results" -type f -name "*-${CHUNK_INDEX}.txt" -printf "%p %s bytes\n" 2>/dev/null || \
            find "results" -type f -name "*-${CHUNK_INDEX}.txt" -exec stat -c '%n %s bytes' {} + 2>/dev/null || true
          
          echo "Sanitizing results for shard ${CHUNK_INDEX}..."
          
          find "results" -type f -name "*-${CHUNK_INDEX}.txt" 2>/dev/null | while read file; do
            echo "  -> Processing $file"
            TEMP_FILE=$(mktemp)
            
            if [[ "$file" == *"massdns-"* ]]; then
              awk 'NF > 0 { $1=$1; print }' "$file" > "$TEMP_FILE"
            else
              awk 'NF > 0 { sub(/^[ \t]+/, ""); sub(/[ \t]+$/, ""); print }' "$file" > "$TEMP_FILE"
            fi
            
            mv "$TEMP_FILE" "$file"
          done
          
          echo "✅ Sanitization complete for shard ${CHUNK_INDEX}."     
          
      - name: Commit Shard-Specific results
        shell: bash
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
          GIT_REPO: ${{ github.repository }}
          GIT_BRANCH: ${{ github.ref_name }}
        run: |
          SOURCE_results_DIR="${GITHUB_WORKSPACE}/results"
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          TMP_DIR=$(mktemp -d)
          echo "Cloning ${GIT_REPO} into temporary directory ${TMP_DIR}..."
          git clone --depth 1 "https://x-access-token:${PAT_TOKEN}@github.com/${GIT_REPO}.git" --branch "$GIT_BRANCH" "$TMP_DIR"
          cd "$TMP_DIR"
          git config user.name "GitHub Actions Bot"
          git config user.email "actions-bot@users.noreply.github.com"
          find "$SOURCE_results_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt' -printf "%p %s bytes\n"
          merge_new_data() {
                    local source_file="$1"
                    local relative_path="${source_file#$SOURCE_results_DIR/}"
                    local dest_file="$TMP_DIR/results/$relative_path"
                    mkdir -p "$(dirname "$dest_file")"
                    TEMP_MERGED=$(mktemp)
                    if [[ "$source_file" == *"massdns-"* ]]; then
                              # For massdns: we expect the source to be already filtered (see bruteforce step).
                              # Overwrite dest with the cleaned source (dedupe defensively).
                              awk '!seen[$0]++' "$source_file" > "$TEMP_MERGED"
                    else
                              # For other result types, sort-unique the source and write it as authoritative.
                              sort -u "$source_file" > "$TEMP_MERGED"
                    fi
                    mv "$TEMP_MERGED" "$dest_file"
          }
          while IFS= read -r -d $'\0' file; do
                    echo "  -> Merging $file into repository's results/ directory"
                    merge_new_data "$file"
          done < <(find "$SOURCE_results_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt'  -print0)
          echo "DEBUG: Files found under SOURCE_results_DIR before merge:"
          find "$SOURCE_results_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt' -printf "%p %s bytes\n" || \
            find "$SOURCE_results_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt' -exec stat -c '%n %s bytes' {} +
          # show first/last 5 lines for large files to inspect unexpected content
          find "$SOURCE_results_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -size +1k -print0 | while IFS= read -r -d '' f; do
                    echo "== $f (head) =="
                    head -n5 "$f" || true
                    echo "== $f (tail) =="
                    tail -n5 "$f" || true
          done
          # repository-side view: what results/ files already exist in the cloned repo
          echo "DEBUG: repository results/ tree (in $TMP_DIR) before merge (ls -l):"
          ls -l "$TMP_DIR/results" 2>/dev/null || true
          git -C "$TMP_DIR" status --porcelain --untracked-files=no || true
          git -C "$TMP_DIR" ls-files -s results/** 2>/dev/null || true
          git add results/
          if git diff --staged --quiet; then
                    echo "No new unique data to commit for shard ${CHUNK_INDEX} after merging."
                    exit 0
          fi
          git commit -m "feat: Update results from shard ${CHUNK_INDEX}"
          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
                    echo "[Attempt $i/$MAX_ATTEMPTS] Pushing results for shard ${CHUNK_INDEX}..."
                    if git pull --rebase origin "$GIT_BRANCH" && git push origin "$GIT_BRANCH"; then
                              echo "✅ Successfully pushed results for shard ${CHUNK_INDEX}."
                              exit 0
                    fi
                    echo "Push failed. Retrying in $(( 5 * i )) seconds..."
                    sleep $(( 5 * i ))
          done
          echo "::error:: All push attempts failed for shard ${CHUNK_INDEX}."
          exit 0
          
      - name: Clone Fresh Repository
        shell: bash
        run: |
          echo "Cloning repository to get committed subdomain_ports files..."
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          
          # Move to parent directory
          cd ..
          REPO_NAME="${{ github.event.repository.name }}"
          
          # Remove if exists
          rm -rf "${REPO_NAME}-httpx"
          
          # Clone fresh copy
          git clone --branch ${{ github.ref_name }} --depth 1 https://github.com/${{ github.repository }}.git "${REPO_NAME}-httpx"
          
          # Verify subdomain_ports files exist
          if [ -d "${REPO_NAME}-httpx/results" ]; then
              FILE_COUNT=$(find "${REPO_NAME}-httpx/results" -type f -name "subdomain_ports-${CHUNK_INDEX}.txt" | wc -l)
              echo "✅ Found $FILE_COUNT subdomain_ports-${CHUNK_INDEX}.txt files"
          fi
          
          # Return to workspace
          cd "$REPO_NAME"
      
      
      - name: Prepare HTTPx Input and Domain Mapping
        shell: bash
        run: |
          
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          REPO_NAME="${{ github.event.repository.name }}"
          CLONED_results="../${REPO_NAME}-httpx/results"
          
          echo "Preparing HTTPx input from subdomain_ports-${CHUNK_INDEX}.txt files..."
          
          mkdir -p httpx-work
          
          HTTPX_INPUT="httpx-work/httpx-input-${CHUNK_INDEX}.txt"
          DOMAIN_MAP="httpx-work/domain-map-${CHUNK_INDEX}.txt"
          
          # Initialize files
          : > "$HTTPX_INPUT"
          : > "$DOMAIN_MAP"
          
          # Find all subdomain_ports files for this chunk across all domains
          find "$CLONED_results" -type f -name "subdomain_ports-${CHUNK_INDEX}.txt" | while read -r file; do
              if [ ! -s "$file" ]; then
                  continue
              fi
              
              # Extract root domain from path: ../repo/results/bmw.com/subdomain_ports-*.txt
              ROOT_DOMAIN=$(echo "$file" | sed "s|.*${CLONED_results}/\([^/]*\)/.*|\1|")
              
              echo "  -> Processing $ROOT_DOMAIN (chunk ${CHUNK_INDEX})"
              
              # Process each line from subdomain_ports file
              while IFS= read -r line || [ -n "$line" ]; do
                  [ -z "$line" ] && continue
                  
                  # Add to httpx input
                  echo "$line" >> "$HTTPX_INPUT"
                  
                  # Create mapping: target root_domain
                  echo "$line $ROOT_DOMAIN" >> "$DOMAIN_MAP"
              done < "$file"
          done
          
          # Deduplicate
          sort -u "$HTTPX_INPUT" -o "$HTTPX_INPUT" || true
          sort -u "$DOMAIN_MAP" -o "$DOMAIN_MAP" || true
          
          TARGET_COUNT=$(wc -l < "$HTTPX_INPUT" 2>/dev/null || echo 0)
          
          if [ "$TARGET_COUNT" -gt 0 ]; then
              echo "✅ Prepared $TARGET_COUNT targets for HTTPx scanning (shard ${{ matrix.shard }})"
          else
              echo "⚠️  No targets found for shard ${{ matrix.shard }}"
          fi
      
      - name: Run HTTPx Scan
        shell: bash
        run: |
          
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          HTTPX_INPUT="httpx-work/httpx-input-${CHUNK_INDEX}.txt"
          HTTPX_OUTPUT="httpx-work/httpx-output-${CHUNK_INDEX}.txt"
          
          if [ ! -s "$HTTPX_INPUT" ]; then
              echo "⚠️  No targets to scan for shard ${{ matrix.shard }}"
              exit 0
          fi
          
          TARGET_COUNT=$(wc -l < "$HTTPX_INPUT" 2>/dev/null || echo 0)
          echo "Running HTTPx on shard ${{ matrix.shard }} with $TARGET_COUNT targets..."
          
          # Run HTTPx with -o flag
          cat "$HTTPX_INPUT" | httpx \
            -status-code \
            -content-length \
            -title \
            -no-color \
            -threads 100 \
            -timeout 10 \
            -random-agent \
            -retries 1 \
            -rate-limit 150 \
            -o "$HTTPX_OUTPUT" \
            -silent
          
          if [ -f "$HTTPX_OUTPUT" ] && [ -s "$HTTPX_OUTPUT" ]; then
              RESULT_COUNT=$(wc -l < "$HTTPX_OUTPUT" 2>/dev/null || echo 0)
              echo "✅ HTTPx scan complete: $RESULT_COUNT live hosts found (shard ${{ matrix.shard }})"
          else
              echo "⚠️  No live hosts found (shard ${{ matrix.shard }})"
              : > "$HTTPX_OUTPUT"
          fi
      
      - name: Distribute HTTPx results to Domain Directories
        shell: bash
        run: |
          
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          HTTPX_OUTPUT="httpx-work/httpx-output-${CHUNK_INDEX}.txt"
          DOMAIN_MAP="httpx-work/domain-map-${CHUNK_INDEX}.txt"
          
          if [ ! -s "$HTTPX_OUTPUT" ]; then
              echo "No HTTPx results to distribute for shard ${{ matrix.shard }}"
              exit 0
          fi
          
          echo "Distributing HTTPx results to domain directories (shard ${{ matrix.shard }})..."
          
          # Create results directory structure
          mkdir -p results
          
          # Process each HTTPx result line
          PROCESSED_COUNT=0
          
          while IFS= read -r line || [ -n "$line" ]; do
              [ -z "$line" ] && continue
              
              # Extract URL (first field)
              URL=$(echo "$line" | awk '{print $1}')
              [ -z "$URL" ] && continue
              
              # Extract host:port from URL (remove protocol)
              HOST_PORT=$(echo "$URL" | sed -E 's|^https?://||')
              
              # Find root domain from mapping
              # Method 1: Exact match with port
              ROOT_DOMAIN=$(awk -v target="$HOST_PORT" '$1 == target {print $2; exit}' "$DOMAIN_MAP")
              
              # Method 2: Match without port
              if [ -z "$ROOT_DOMAIN" ]; then
                  HOST_ONLY=$(echo "$HOST_PORT" | cut -d: -f1)
                  ROOT_DOMAIN=$(awk -v target="$HOST_ONLY" '$1 == target {print $2; exit}' "$DOMAIN_MAP")
              fi
              
              # Method 3: Pattern match on subdomain
              if [ -z "$ROOT_DOMAIN" ]; then
                  HOST_ONLY=$(echo "$HOST_PORT" | cut -d: -f1)
                  ROOT_DOMAIN=$(awk -v host="$HOST_ONLY" '
                    {
                      domain = $2
                      if (host == domain || host ~ "\\." domain "$") {
                        print domain
                        exit
                      }
                    }
                  ' "$DOMAIN_MAP")
              fi
              
              if [ -n "$ROOT_DOMAIN" ]; then
                  # Create domain directory
                  mkdir -p "results/$ROOT_DOMAIN"
                  
                  # Append URL to domain's httpx file for this chunk
                  echo "$URL" >> "results/$ROOT_DOMAIN/httpx_results-${CHUNK_INDEX}.txt"
                  PROCESSED_COUNT=$((PROCESSED_COUNT + 1))
              fi
          done < "$HTTPX_OUTPUT"
          
          # Deduplicate each domain's httpx results file
          find results -type f -name "httpx_results-${CHUNK_INDEX}.txt" | while read -r file; do
              sort -u "$file" -o "$file"
          done
          
          echo "✅ Distributed $PROCESSED_COUNT URLs to domain directories (shard ${{ matrix.shard }})"
          
      - name: Commit HTTPx results
        shell: bash
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
          GIT_REPO: ${{ github.repository }}
          GIT_BRANCH: ${{ github.ref_name }}
        run: |
          SOURCE_results_DIR="${GITHUB_WORKSPACE}/results"
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          TMP_DIR=$(mktemp -d)
          echo "Cloning ${GIT_REPO} into temporary directory ${TMP_DIR} for HTTPx results..."
          git clone --depth 1 "https://x-access-token:${PAT_TOKEN}@github.com/${GIT_REPO}.git" --branch "$GIT_BRANCH" "$TMP_DIR"
          cd "$TMP_DIR"
          git config user.name "GitHub Actions Bot"
          git config user.email "actions-bot@users.noreply.github.com"
          # This function is for merging the new data into the repository
          merge_new_data() {
              local source_file="$1"
              local relative_path="${source_file#$SOURCE_results_DIR/}"
              local dest_file="$TMP_DIR/results/$relative_path"
              mkdir -p "$(dirname "$dest_file")"
              # Append new results and then sort unique
              cat "$source_file" >> "$dest_file.tmp"
              sort -u "$dest_file.tmp" > "$dest_file"
              rm "$dest_file.tmp"
          }
          # Find and process only the httpx_results files
          while IFS= read -r -d $'\0' file; do
              echo "  -> Merging $file into repository's results/ directory"
              merge_new_data "$file"
          done < <(find "$SOURCE_results_DIR" -type f -name "httpx_results-${CHUNK_INDEX}.txt" -print0)
          git add results/
          if git diff --staged --quiet; then
              echo "No new unique httpx data to commit for shard ${CHUNK_INDEX}."
              exit 0
          fi
          git commit -m "feat: Add httpx results from shard ${CHUNK_INDEX}"
          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
              echo "[Attempt $i/$MAX_ATTEMPTS] Pushing httpx results for shard ${CHUNK_INDEX}..."
              if git pull --rebase origin "$GIT_BRANCH" && git push origin "$GIT_BRANCH"; then
                  echo "✅ Successfully pushed httpx results for shard ${CHUNK_INDEX}."
                  exit 0
              fi
              echo "Push failed. Retrying in $(( 5 * i )) seconds..."
              sleep $(( 5 * i ))
          done
          echo "::error:: All push attempts failed for httpx results from shard ${CHUNK_INDEX}."
          exit 0
          
      - name: Upload HTTPx results for This Shard
        uses: actions/upload-artifact@v4
        with:
          name: httpx-results-${{ matrix.shard }}
          path: results/**/httpx_results-*.txt
          retention-days: 1      

  summarize_results:
    name: Summarize and Commit Result Counts
    runs-on: ubuntu-latest
    needs: [prepare, brute]
    if: always() && needs.prepare.result == 'success'

    steps:
      - name: Checkout final repository state
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT_TOKEN }}

      - name: Ensure gawk present (required for PROCINFO["sorted_in"])
        shell: bash
        run: |
          if ! command -v gawk >/dev/null 2>&1; then
            echo "gawk not found — installing..."
            sudo apt-get update -y
            sudo apt-get install -y gawk
          else
            echo "gawk present"
          fi

      - name: Generate and Commit Summary
        shell: bash
        run: |
          # Variables
          BRANCH="${{ github.ref_name }}"
          RESULTS_DIR="results"
          OUTPUT_FILE="count_result.txt"

          echo "Configuring Git..."
          git config user.name "GitHub Actions Bot"
          git config user.email "actions-bot@users.noreply.github.com"

          # Try to fetch origin refs safely; tolerate failure
          git fetch --prune origin '+refs/heads/*:refs/remotes/origin/*' || true

          # Checkout a local branch that tracks origin/<branch> where possible
          if git rev-parse --verify "origin/$BRANCH" >/dev/null 2>&1; then
            git checkout -B "$BRANCH" "origin/$BRANCH" || git checkout -B "$BRANCH"
          else
            git checkout -B "$BRANCH" || true
          fi

          # Try to rebase/autostash to get latest; don't abort on failure
          git pull --rebase --autostash origin "$BRANCH" || echo "Note: git pull --rebase returned non-zero; continuing."

          echo "--- Generating result count summary with per-domain totals ---"
          if [ ! -d "$RESULTS_DIR" ]; then
            echo "⚠️  Results directory not found. Nothing to summarize."
            exit 0
          fi

          # Use -print0 + gawk with RS set inside the program to safely handle arbitrary filenames
          find "$RESULTS_DIR" -type f -name "*.txt" -not -name 'wildcards-*.txt' -print0 | \
          gawk '
            BEGIN {
              # Set RS to NUL character safely
              RS = sprintf("%c", 0)
            }
            {
              filepath = $0
              # Get line count via shell wc -l. Use quoted filepath to be safe.
              cmd = "wc -l < \"" filepath "\""
              cmd | getline cnt
              close(cmd)
              # Normalize whitespace -> number
              gsub(/^ +| +$/, "", cnt)
              if (cnt == "") cnt = 0
              count = cnt + 0

              # split path parts
              n = split(filepath, parts, "/")

              # Find "results" in path and pick next element as domain if present
              domain = ""
              for (i = 1; i <= n; i++) {
                if (parts[i] == "results") {
                  if (i + 1 <= n) domain = parts[i+1]
                  break
                }
              }
              if (domain == "") {
                # fallback to second-to-last element (original heuristic)
                domain = parts[n-1]
              }

              filename = parts[n]
              # normalize pattern: strip .txt, then optional -<digits>
              sub(/\.txt$/, "", filename)
              sub(/-[0-9]+$/, "", filename)
              pattern = filename

              individual_counts[domain, filepath] = count
              domain_totals[domain, pattern] += count
              domains_seen[domain] = 1
              patterns_seen[pattern] = 1
            }
            END {
              # stable sort for domain iteration
              PROCINFO["sorted_in"] = "@ind_str_asc"

              # Preferred order for totals (keeps original behavior)
              split("httpx_results massdns puredns-output subdomain_ports", preferred, " ")

              for (domain in domains_seen) {
                print "--- Domain: " domain " ---"
                # Print preferred patterns in defined order
                for (i = 1; i <= length(preferred); i++) {
                  p = preferred[i]
                  if (domain_totals[domain, p] > 0) {
                    printf "  TOTAL %-20s: %d\n", p, domain_totals[domain, p]
                  }
                }
                # Print any other discovered patterns (sorted)
                PROCINFO["sorted_in"] = "@ind_str_asc"
                for (p in patterns_seen) {
                  # skip preferred ones
                  skip = 0
                  for (j = 1; j <= length(preferred); j++) if (preferred[j] == p) skip = 1
                  if (skip) continue
                  if (domain_totals[domain, p] > 0) {
                    printf "  TOTAL %-20s: %d\n", p, domain_totals[domain, p]
                  }
                }
                print ""
              }

              print "\n--- Individual File Counts ---"
              PROCINFO["sorted_in"] = "@ind_str_asc"
              for (domain in domains_seen) {
                for (key in individual_counts) {
                  split(key, kparts, SUBSEP)
                  kdomain = kparts[1]
                  if (kdomain != domain) continue
                  filepath = kparts[2]
                  m = split(filepath, fp, "/")
                  fname = fp[m]
                  cnt = individual_counts[key]
                  if (cnt > 0) print domain ":- " fname, cnt
                }
              }
            }
          ' > "$OUTPUT_FILE"

          # If output missing/empty -> nothing to commit
          if [ ! -s "$OUTPUT_FILE" ]; then
            echo "Summary file is empty or missing. No commit needed."
            exit 0
          fi

          git add "$OUTPUT_FILE" || true

          # If nothing staged, exit cleanly
          if git diff --staged --quiet --exit-code; then
            echo "Result summary is unchanged. No commit needed."
            exit 0
          fi

          git commit -m "chore: Update result count summary" || echo "No commit created (maybe nothing staged)."

          MAX_ATTEMPTS=3
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
            echo "[Attempt $i/$MAX_ATTEMPTS] Pushing result summary..."
            git pull --rebase --autostash origin "$BRANCH" || echo "Note: pull returned non-zero; continuing to push attempt."
            if git push origin "$BRANCH"; then
              echo "✅ Successfully pushed result summary."
              exit 0
            fi
            echo "Push failed. Retrying in 5 seconds..."
            sleep 5
          done

          echo "::error:: All push attempts for the summary file failed (non-fatal)."
          # Ensure job completes successfully — change to `exit 1` here if you'd prefer fail-on-push.
          exit 0
  
  dispatch-next:
    name: dispatch-next-run
    runs-on: ubuntu-latest
    
    needs: [prepare, coordinate_and_dispatch, brute]
    if: always()
    env:
      PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
    steps:
      - name: Compute and dispatch next run
        shell: bash
        run: |
          if [ -z "${PAT_TOKEN}" ]; then echo "Error: secrets.PAT_TOKEN is required." >&2; exit 0; fi
          
          # THIS IS THE KEY CHANGE: Get the next starting index from our coordinator job
          NEXT_START=${{ needs.coordinate_and_dispatch.outputs.next_recursive_start }}
          
          TOTAL_CHUNKS=${{ needs.prepare.outputs.total-chunks }}
          RUN_COUNTER=${{ github.event.inputs.run_counter }}
          MAX_RUNS=${{ needs.prepare.outputs.effective-max-runs }}
          
          NEXT_COUNTER=$((RUN_COUNTER + 1))
          
          echo "Current primary start: ${{ github.event.inputs.chunk_start }}, Next recursive start: ${NEXT_START}, Total chunks: ${TOTAL_CHUNKS}"
          
          if [ "$NEXT_START" -ge "$TOTAL_CHUNKS" ]; then echo "All chunks processed. Terminating."; exit 0; fi
          if [ "${NEXT_COUNTER}" -ge "${MAX_RUNS}" ]; then echo "Next run would exceed max_runs limit. Terminating."; exit 0; fi
          
          URL="https://api.github.com/repos/${{ github.repository }}/actions/workflows/${{ env.WORKFLOW_FILE }}/dispatches"
          BODY=$(printf '{"ref":"%s","inputs":{"chunk_start":"%s","run_counter":"%s"}}' \
            "${{ github.ref_name }}" "${NEXT_START}" "${NEXT_COUNTER}")
            
          echo "Dispatching next run..."
          for i in 1 2 3 4; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" -X POST -H "Accept: application/vnd.github+json" \
              -H "Authorization: Bearer ${PAT_TOKEN}" -H "X-GitHub-Api-Version: 2022-11-28" -d "$BODY" "$URL")
            if [ "$HTTP_STATUS" -ge 200 ] && [ "$HTTP_STATUS" -lt 300 ]; then echo "Dispatch succeeded (HTTP ${HTTP_STATUS})"; exit 0; fi
            echo "Dispatch attempt ${i} failed (HTTP ${HTTP_STATUS}). Retrying..."
            sleep $((i * 2))
          done
          echo "Failed to dispatch next run after retries. Terminating with an error."
          exit 0
