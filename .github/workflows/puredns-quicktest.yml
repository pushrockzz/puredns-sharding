name: puredns-quick-test

on:
  workflow_dispatch:
    inputs:
      chunk_start:
        description: 'First chunk index this run will process (integer)'
        required: true
        default: '0'
      total_chunks:
        description: 'DEPRECATED: Total chunks is now auto-detected.'
        required: false
      run_counter:
        description: 'Automatic run counter for recursion (auto-incremented)'
        required: false
        default: '0'
      max_runs:
        description: 'Safety cap: stop after this many runs. If unset, it is auto-calculated.'
        required: false
        default: '999'
      words_per_chunk:
        description: 'Words per chunk file (lines_per_chunk).'
        required: false
        default: '40000'
      domains_count:
        description: 'How many test domains to generate (default = 5)'
        required: false
        default: '5'

env:
  WORKFLOW_FILE: puredns-quicktest.yml
  MATRIX_SIZE: '20'

jobs:
  prepare:
    name: prepare-wordlists-and-resolvers
    runs-on: ubuntu-latest
    env:
      COMBINED_WORDLIST: combined-wordlist.txt
    outputs:
      artifact-name: wordlists-artifact
      total-chunks: ${{ steps.finalize_prep.outputs.total-chunks }}
      effective-max-runs: ${{ steps.finalize_prep.outputs.effective-max-runs }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: false
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'
      - name: Install Go tools
        run: go install -v github.com/tomnomnom/anew@latest
      - name: Cache Raw Wordlists
        uses: actions/cache@v3
        with:
          path: |
            best-wordlist-level1.txt
            best-wordlist-level2.txt
          key: raw-wordlists-cache-v1-${{ runner.os }}
      - name: Fetch wordlists and Resolvers
        run: |
          if [ ! -f best-wordlist-level1.txt ]; then wget -qO best-wordlist-level1.txt https://raw.githubusercontent.com/trickest/wordlists/main/inventory/levels/level1.txt; fi
          if [ ! -f best-wordlist-level2.txt ]; then wget -qO best-wordlist-level2.txt https://raw.githubusercontent.com/trickest/wordlists/main/inventory/levels/level2.txt; fi
          wget -qO resolvers.txt https://raw.githubusercontent.com/rix4uni/resolvers/main/resolvers.txt
          wget -qO resolvers-trusted.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/main/resolvers.txt
      - name: Build filter_wordlist tool
        run: |
          if [ -f "filter_wordlist.go" ]; then go build -o filter_wordlist filter_wordlist.go; fi

      - name: Cache Filtered Wordlist
        id: cache-filtered-wordlist
        uses: actions/cache@v3
        with:
          path: ${{ env.COMBINED_WORDLIST }}
          key: filtered-wordlist-cache-v1-${{ runner.os }}-${{ hashFiles('best-wordlist-*.txt', 'filter_wordlist.go') }}
      - name: Filter and Combine Wordlists
        if: steps.cache-filtered-wordlist.outputs.cache-hit != 'true'
        run: |
          if [ -f ./filter_wordlist ]; then
            ./filter_wordlist best-wordlist-level1.txt > best-wordlist-filtered.txt
            ./filter_wordlist best-wordlist-level2.txt > best-wordlist-level2-filtered.txt
          else
            cp best-wordlist-level1.txt best-wordlist-filtered.txt
            cp best-wordlist-level2.txt best-wordlist-level2-filtered.txt
          fi
          cat best-wordlist-filtered.txt best-wordlist-level2-filtered.txt | PATH=$HOME/go/bin:$PATH anew -q ${{ env.COMBINED_WORDLIST }}
      - name: Finalize Preparation and Split Chunks
        id: finalize_prep
        run: |
          mkdir -p domains wordlists results
          if [ ! -f "domains/domains.txt" ]; then
            for i in $(seq 1 "${{ github.event.inputs.domains_count }}"); do echo "test${i}.example.com"; done > "domains/domains.txt"
          fi
          COMBINED_WORDLIST="${{ env.COMBINED_WORDLIST }}"
          if [ ! -s "$COMBINED_WORDLIST" ]; then echo "::error:: Combined wordlist is empty."; exit 0; fi
          rm -f wordlists/chunk-*.txt || true
          split -l "${{ github.event.inputs.words_per_chunk }}" -d -a 5 --additional-suffix=.txt "$COMBINED_WORDLIST" wordlists/chunk-
          if [ -z "$(ls -A wordlists)" ]; then echo "::error:: Split failed to create chunks."; exit 0; fi
          CHUNK_COUNT=$(ls -1 wordlists/chunk-*.txt | wc -l)
          echo "total-chunks=${CHUNK_COUNT}" >> $GITHUB_OUTPUT
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          CALCULATED_RUNS=$(( (CHUNK_COUNT + MATRIX_SIZE - 1) / MATRIX_SIZE ))
          USER_MAX_RUNS=${{ github.event.inputs.max_runs }}
          EFFECTIVE_MAX_RUNS=$(( CALCULATED_RUNS < USER_MAX_RUNS ? CALCULATED_RUNS : USER_MAX_RUNS ))
          echo "effective-max-runs=${EFFECTIVE_MAX_RUNS}" >> $GITHUB_OUTPUT
      
      - name: Upload binaries and wordlists as artifact
        uses: actions/upload-artifact@v4
        with:
          name: wordlists-artifact
          path: |
            wordlists
            resolvers.txt
            resolvers-trusted.txt
            domains/domains.txt


  generate_matrix:
    name: Generate Dynamic Matrix
    runs-on: ubuntu-latest
    needs: prepare
    outputs:
      shard_matrix: ${{ steps.gen_matrix.outputs.shard_matrix }}
    steps:
      - name: Calculate shards for this run
        id: gen_matrix
        run: |
          TOTAL_CHUNKS=${{ needs.prepare.outputs.total-chunks }}
          CHUNK_START=${{ github.event.inputs.chunk_start }}
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          CHUNKS_REMAINING=$(( TOTAL_CHUNKS - CHUNK_START ))
          if (( CHUNKS_REMAINING < 0 )); then CHUNKS_REMAINING=0; fi
          SHARDS_THIS_RUN=$(( CHUNKS_REMAINING < MATRIX_SIZE ? CHUNKS_REMAINING : MATRIX_SIZE ))
          if (( SHARDS_THIS_RUN <= 0 )); then
            echo "shard_matrix=[]" >> $GITHUB_OUTPUT
          else
            MATRIX_JSON=$(jq -cn --argjson n "$SHARDS_THIS_RUN" '[range($n)]')
            echo "Generated matrix for ${SHARDS_THIS_RUN} shards: $MATRIX_JSON"
            echo "shard_matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT
          fi

  brute:
    name: bruteforce-shard (shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    needs: [prepare, generate_matrix]
    if: needs.generate_matrix.outputs.shard_matrix != '[]'
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      fail-fast: false
      matrix:
        shard: ${{ fromJson(needs.generate_matrix.outputs.shard_matrix) }}
    steps:
      - name: Checkout repo (for committing results)
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0   
          
      - name: Download artifacts (binaries and wordlists)
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare.outputs.artifact-name }}
          path: .

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

      - name: Install Tools
        run: |

          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi    
          
      - name: Run puredns bruteforce
        shell: bash
        run: |
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          CHUNK_FILE="wordlists/chunk-$(printf '%05d' "$CHUNK_INDEX").txt"
          if [ ! -f "$CHUNK_FILE" ]; then exit 0; fi
          while IFS= read -r domain || [ -n "$domain" ]; do
            case "$domain" in \#*|"") continue ;; esac
            ROOT_DOMAIN_DIR="results/$domain"
            mkdir -p "$ROOT_DOMAIN_DIR"
            cat "$CHUNK_FILE" | puredns bruteforce "$domain" -r "resolvers.txt" --rate-limit 5000 --rate-limit-trusted 2000 \
              --resolvers-trusted "resolvers-trusted.txt" --wildcard-tests 300 --wildcard-batch 100000 \
              --write "${ROOT_DOMAIN_DIR}/puredns-output-${CHUNK_INDEX}.txt" \
              --write-wildcards "${ROOT_DOMAIN_DIR}/wildcards-${CHUNK_INDEX}.txt" \
              --write-massdns "${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt" --quiet


            PUREDNS_FILE="${ROOT_DOMAIN_DIR}/puredns-output-${CHUNK_INDEX}.txt"
            MASSDNS_FILE="${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt"
          
            # per-domain tmpdir for isolation
            SAFE_DOMAIN_NAME=$(echo "$domain" | sed 's/[^A-Za-z0-9._-]/_/g')
            DOMAIN_TMPDIR=$(mktemp -d -p "$(pwd)" "tmp-${SAFE_DOMAIN_NAME}-XXXX")
            TMP_PURE="$DOMAIN_TMPDIR/puredns-valid.txt"
            TMP_CLEAN="$DOMAIN_TMPDIR/massdns-clean.txt"
            TMP_FILTER="$DOMAIN_TMPDIR/massdns-filtered.txt"
          
            # Build normalized PureDNS host list (strip trailing dot, lowercase, unique) if present
            if [ -s "$PUREDNS_FILE" ]; then
              awk '{ gsub(/\.$/,"",$1); print tolower($1) }' "$PUREDNS_FILE" | sort -u > "$TMP_PURE"
              PURE_COUNT=$(wc -l < "$TMP_PURE" 2>/dev/null || echo 0)
            else
              PURE_COUNT=0
              : > "$TMP_PURE"
            fi
          
            # If massdns exists, clean it into canonical lines "host A ip"
            if [ -s "$MASSDNS_FILE" ]; then
              awk 'NF {
                # normalize host (first field), find first "A" token (any field) and take next field as IP
                host=$1; sub(/\.$/,"",host);
                aidx=0
                for(i=2;i<=NF;i++){
                  if($i=="A"){ aidx=i; break }
                }
                if(aidx>0 && (aidx+1)<=NF){
                  ip=$(aidx+1);
                  gsub(/^[ \t]+|[ \t]+$/,"",host);
                  gsub(/^[ \t]+|[ \t]+$/,"",ip);
                  if(host!="" && ip!="") print host " A " ip;
                }
              }' "$MASSDNS_FILE" | sort -u > "$TMP_CLEAN" || true
            else
              # no massdns produced (create empty clean file)
              : > "$TMP_CLEAN"
            fi
          
            # Filter cleaned massdns against puredns hostlist if puredns exists; otherwise keep cleaned entries
            if [ -s "$TMP_PURE" ]; then
              awk -v validfile="$TMP_PURE" '
                BEGIN {
                  while((getline v < validfile) > 0) valid[tolower(v)] = 1
                  close(validfile)
                }
                NF>=3 {
                  host=$1; host_l=tolower(host);
                  if(host_l in valid) print $0
                }
              ' "$TMP_CLEAN" > "$TMP_FILTER" || true
            else
              cp "$TMP_CLEAN" "$TMP_FILTER" || true
            fi
          
            # Atomic replace: move filtered output back to domain massdns file
            mv "$TMP_FILTER" "$MASSDNS_FILE"
            rm -f "$TMP_CLEAN"
          
            # Count massdns lines for echo (safe even if empty)
            MASSDNS_COUNT=$(wc -l < "$MASSDNS_FILE" 2>/dev/null || echo 0)
          
            # Echo exact messages you requested
            echo "✅ PureDNS resolution complete. ${PURE_COUNT} subdomains were successfully resolved."
            echo "✅ PureDNS resolution complete. ${MASSDNS_COUNT} massdns found."
          
            # remove per-domain tmpdir
            rm -rf "$DOMAIN_TMPDIR"
          
          done < "domains/domains.txt"

      - name: Perform Port Scanning
        shell: bash
        run: |
          # Disable exit on error to handle failures gracefully
                   
          PORTS="1,43,49,70,79,80,81,82,83,84,85,88,102,104,113,135,139,143,175,179,195,264,280,389,443,444,505,515,548,554,591,631,771,783,789,888,898,900,901,993,995,1026,1080,1099,1153,1177,1200,1214,1220,1234,1311,1314,1344,1433,1503,1515,1521,1599,1723,1830,1900,1962,2000,2001,2002,2030,2064,2081,2087,2181,2222,2306,2345,2404,2455,2525,2715,2761,2762,3000,3001,3002,3052,3128,3260,3299,3310,3388,3389,3460,3531,3689,4000,4157,4242,4369,4443,4444,4500,4567,4711,4786,4899,5000,5001,5007,5009,5010,5025,5060,5222,5269,5280,5427,5432,5672,5800,5801,5802,5900,5938,6000,6001,6103,6346,6544,6600,6668,6699,6969,7002,7007,7070,7100,7171,7415,7776,8000,8001,8002,8003,8004,8005,8006,8007,8008,8009,8010,8080,8081,8082,8083,8084,8085,8087,8088,8118,8126,8181,8291,8443,8880,8881,8882,8883,8884,8885,8886,8887,8888,9000,9001,9002,9003,9030,9050,9080,9090,9100,9530,9600,9633,9999,10000,10001,10005,10134,11112,12345,13013,13666,15000,18245,20000,20256,20547,21379,25001,25565,31337,35000,37777,44818,50000,54138,55000,55555,60129"
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          
          # Create a single, global temporary directory for this entire step's operations
          GLOBAL_TMPDIR=$(mktemp -d)
          # This 'trap' command ensures the temporary directory is automatically deleted when the step exits
          trap 'rm -rf "$GLOBAL_TMPDIR"' EXIT

          # --- PHASE 1: AGGREGATE ALL TARGETS ---
          echo "PHASE 1: Aggregating all targets from all domains..."
          
          # Define paths for our new master files inside the safe temporary directory
          MASTER_IP_TO_SUBDOMAIN_MAP="${GLOBAL_TMPDIR}/master_ip_to_subdomain.txt"
          MASTER_UNIQUE_IPS="${GLOBAL_TMPDIR}/master_unique_ips.txt"

          # Initialize empty files
          : > "$MASTER_IP_TO_SUBDOMAIN_MAP"

          # Loop through domains for the sole purpose of collecting IPs and mapping them.
          while IFS= read -r domain || [ -n "$domain" ]; do
              case "$domain" in \#*|"") continue ;; esac
              ROOT_DOMAIN_DIR="results/$domain"
              MASSDNS_INPUT_FILE="${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt"

              if [ ! -s "$MASSDNS_INPUT_FILE" ]; then
                  continue
              fi

              # Read the massdns file, clean it, and create a map of "IP SUBDOMAIN".
              # Append this to our master map file. This preserves the crucial link.
              awk 'NF>=3 && $2=="A" { host=$1; sub(/\.$/,"",host); print $3, host }' "$MASSDNS_INPUT_FILE" \
                | sort -u >> "$MASTER_IP_TO_SUBDOMAIN_MAP" || true
          done < "domains/domains.txt"

          if [ ! -s "$MASTER_IP_TO_SUBDOMAIN_MAP" ]; then
              echo "⚠️  No IPs found across any domains. Exiting port scanning step."
              exit 0
          fi

          # Create the master list of unique IPs to be scanned from our map.
          cut -d' ' -f1 "$MASTER_IP_TO_SUBDOMAIN_MAP" | sort -u > "$MASTER_UNIQUE_IPS" || true
          echo "✅ Phase 1 Complete: Collected $(wc -l < "$MASTER_UNIQUE_IPS" 2>/dev/null || echo 0) unique IPs from $(wc -l < "$MASTER_IP_TO_SUBDOMAIN_MAP" 2>/dev/null || echo 0) total mappings."

          # --- PHASE 2: PERFORM A SINGLE, CONSOLIDATED SCAN ---
          echo "PHASE 2: Performing one consolidated scan..."

          MASTER_NON_CDN_IPS="${GLOBAL_TMPDIR}/master_non_cdn.txt"
          MASTER_SCAN_RESULTS="${GLOBAL_TMPDIR}/master_scan_results.txt"
          
          # Initialize empty file
          : > "$MASTER_NON_CDN_IPS"
          
          # Run CDN check ONCE on the master IP list
          cat "$MASTER_UNIQUE_IPS" | cut-cdn -ua -t 50 -silent -o "$MASTER_NON_CDN_IPS" || true

          # If any non-CDN IPs exist, run the scanners ONCE
          if [ -s "$MASTER_NON_CDN_IPS" ]; then
              echo "  -> Scanning $(wc -l < "$MASTER_NON_CDN_IPS" 2>/dev/null || echo 0) non-CDN IPs..."
              TMP_NAABU_RUSTSCAN=$(mktemp)
              TMP_RUSTSCAN=$(mktemp)
              
              # Your original tool commands, unchanged
              naabu -l "$MASTER_NON_CDN_IPS" -passive -o "$TMP_NAABU_RUSTSCAN" -no-color -silent || true
              rustscan -a "$MASTER_NON_CDN_IPS" -p "$PORTS" --no-banner -t 5000 -b 300 --greppable > "$TMP_RUSTSCAN" || true
              
              # Your original processing logic, unchanged
              cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }' | PATH=$HOME/go/bin:$PATH anew -q "$TMP_NAABU_RUSTSCAN" || true
              
              # Process scan results into consistent format: "IP PORT"
              awk -F: 'NF==2 { print $1, $2 } NF==1 { print $1, "NOPORT" }' "$TMP_NAABU_RUSTSCAN" | sort -u > "$MASTER_SCAN_RESULTS" || true
              rm -f "$TMP_RUSTSCAN" "$TMP_NAABU_RUSTSCAN" 2>/dev/null || true
          else
              # Create empty scan results file
              : > "$MASTER_SCAN_RESULTS"
          fi
          
          # Add CDN IPs back to the results list with NOPORT marker for consistency
          MASTER_CDN_IPS="${GLOBAL_TMPDIR}/master_cdn_ips.txt"
          comm -13 <(sort "$MASTER_NON_CDN_IPS" 2>/dev/null || true) <(sort "$MASTER_UNIQUE_IPS" 2>/dev/null || true) > "$MASTER_CDN_IPS" 2>/dev/null || true
          if [ -s "$MASTER_CDN_IPS" ]; then
              awk '{ print $1, "NOPORT" }' "$MASTER_CDN_IPS" >> "$MASTER_SCAN_RESULTS" || true
          fi
          
          echo "✅ Phase 2 Complete: Scan finished ($(wc -l < "$MASTER_SCAN_RESULTS" 2>/dev/null || echo 0) total IP-port combinations)."

          # --- PHASE 3: DISTRIBUTE RESULTS ---
          echo "PHASE 3: Distributing results to domain directories..."
          
          # Sort master files once for efficient processing
          MASTER_SCAN_RESULTS_SORTED="${GLOBAL_TMPDIR}/master_scan_sorted.txt"
          MASTER_MAP_SORTED="${GLOBAL_TMPDIR}/master_map_sorted.txt"
          sort -k1,1 "$MASTER_SCAN_RESULTS" > "$MASTER_SCAN_RESULTS_SORTED" 2>/dev/null || true
          sort -k1,1 "$MASTER_IP_TO_SUBDOMAIN_MAP" > "$MASTER_MAP_SORTED" 2>/dev/null || true
          
          # Verify sorted files exist
          if [ ! -f "$MASTER_SCAN_RESULTS_SORTED" ] || [ ! -f "$MASTER_MAP_SORTED" ]; then
              echo "⚠️  Error: Failed to create sorted master files. Exiting Phase 3."
              exit 0
          fi
          
          # Loop through domains a final time to generate their specific report files
          DOMAINS_PROCESSED=0
          DOMAINS_SKIPPED=0
          
          while IFS= read -r domain || [ -n "$domain" ]; do
              case "$domain" in \#*|"") continue ;; esac
              ROOT_DOMAIN_DIR="results/$domain"
              OUTPUT_FILE="${ROOT_DOMAIN_DIR}/subdomain_ports-${CHUNK_INDEX}.txt"
              
              # Escape domain for safe regex matching - only escape dots for regex
              ESCAPED_DOMAIN=$(printf '%s\n' "$domain" | sed 's/\./\\./g' 2>/dev/null || echo "$domain")
              
              # Extract only the map entries for this specific domain
              # Pattern: matches any line where second field ends with .domain.com OR is exactly domain.com
              # The map format is: "IP SUBDOMAIN", so we match on the SUBDOMAIN field
              DOMAIN_MAP_TMP=$(mktemp)
              awk -v domain="$domain" '
                $2 == domain || $2 ~ "\\." domain "$" { print $0 }
              ' "$MASTER_MAP_SORTED" > "$DOMAIN_MAP_TMP" 2>/dev/null || true

              if [ ! -s "$DOMAIN_MAP_TMP" ]; then
                  echo "  -> ⚠️  Skipping $domain (no IPs found in master map)"
                  DOMAINS_SKIPPED=$((DOMAINS_SKIPPED + 1)) || true
                  rm -f "$DOMAIN_MAP_TMP" 2>/dev/null || true
                  continue
              fi
              
              # Join scan results with this domain's IP-subdomain map
              # Output format after join: IP PORT SUBDOMAIN
              join -t' ' -1 1 -2 1 "$MASTER_SCAN_RESULTS_SORTED" "$DOMAIN_MAP_TMP" 2>/dev/null | \
                awk '{
                    ip = $1
                    port = $2
                    subdomain = $3
                    
                    # If port exists and is not NOPORT marker, format as subdomain:port
                    if (port != "NOPORT" && port ~ /^[0-9]+$/) {
                        print subdomain ":" port
                    } else {
                        # No port data (CDN IP), just print subdomain
                        print subdomain
                    }
                  }' 2>/dev/null | \
                sort -u > "$OUTPUT_FILE" 2>/dev/null || true

              if [ -s "$OUTPUT_FILE" ]; then
                  LINE_COUNT=$(wc -l < "$OUTPUT_FILE" 2>/dev/null || echo 0)
                  echo "  -> ✅ Generated $OUTPUT_FILE for $domain ($LINE_COUNT entries)"
                  DOMAINS_PROCESSED=$((DOMAINS_PROCESSED + 1)) || true
              else
                  echo "  -> ⚠️  Warning: $domain has no port scan results (empty output)"
                  DOMAINS_SKIPPED=$((DOMAINS_SKIPPED + 1)) || true
              fi
              
              rm -f "$DOMAIN_MAP_TMP" 2>/dev/null || true
          done < "domains/domains.txt"
          
          echo "✅ Phase 3 Complete: $DOMAINS_PROCESSED domains processed, $DOMAINS_SKIPPED domains skipped."
          
          # Explicitly exit with success
          exit 0
          
      - name: Sanitize and Finalize Results
        shell: bash
        run: |
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          echo "DEBUG: matrix.shard='${{ matrix.shard }}' chunk_start='${{ github.event.inputs.chunk_start }}' CHUNK_INDEX=$CHUNK_INDEX"
          
          # show any pre-existing result files for this chunk (if any)
          find "results" -type f -name "*-${CHUNK_INDEX}.txt" -printf "%p %s bytes\n" 2>/dev/null || \
            find "results" -type f -name "*-${CHUNK_INDEX}.txt" -exec stat -c '%n %s bytes' {} + 2>/dev/null || true
          
          echo "Sanitizing results for shard ${CHUNK_INDEX}..."
          
          find "results" -type f -name "*-${CHUNK_INDEX}.txt" 2>/dev/null | while read file; do
            echo "  -> Processing $file"
            TEMP_FILE=$(mktemp)
            
            if [[ "$file" == *"massdns-"* ]]; then
              awk 'NF > 0 { $1=$1; print }' "$file" > "$TEMP_FILE"
            else
              awk 'NF > 0 { sub(/^[ \t]+/, ""); sub(/[ \t]+$/, ""); print }' "$file" > "$TEMP_FILE"
            fi
            
            mv "$TEMP_FILE" "$file"
          done
          
          echo "✅ Sanitization complete for shard ${CHUNK_INDEX}."     

      - name: Commit Shard-Specific Results
        shell: bash
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
          GIT_REPO: ${{ github.repository }}
          GIT_BRANCH: ${{ github.ref_name }}
        run: |
          SOURCE_RESULTS_DIR="${GITHUB_WORKSPACE}/results"
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          TMP_DIR=$(mktemp -d)
          echo "Cloning ${GIT_REPO} into temporary directory ${TMP_DIR}..."
          git clone --depth 1 "https://x-access-token:${PAT_TOKEN}@github.com/${GIT_REPO}.git" --branch "$GIT_BRANCH" "$TMP_DIR"
          cd "$TMP_DIR"
          git config user.name "GitHub Actions Bot"
          git config user.email "actions-bot@users.noreply.github.com"
          find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt' -not -name 'puredns-output-*.txt' -printf "%p %s bytes\n"
          merge_new_data() {
                    local source_file="$1"
                    local relative_path="${source_file#$SOURCE_RESULTS_DIR/}"
                    local dest_file="$TMP_DIR/results/$relative_path"
                    mkdir -p "$(dirname "$dest_file")"
                    TEMP_MERGED=$(mktemp)
                    if [[ "$source_file" == *"massdns-"* ]]; then
                              # For massdns: we expect the source to be already filtered (see bruteforce step).
                              # Overwrite dest with the cleaned source (dedupe defensively).
                              awk '!seen[$0]++' "$source_file" > "$TEMP_MERGED"
                    else
                              # For other result types, sort-unique the source and write it as authoritative.
                              sort -u "$source_file" > "$TEMP_MERGED"
                    fi
                    mv "$TEMP_MERGED" "$dest_file"
          }
          while IFS= read -r -d $'\0' file; do
                    echo "  -> Merging $file into repository's results/ directory"
                    merge_new_data "$file"
          done < <(find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt' -not -name 'puredns-output-*.txt' -print0)
          echo "DEBUG: Files found under SOURCE_RESULTS_DIR before merge:"
          find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt' -not -name 'puredns-output-*.txt' -printf "%p %s bytes\n" || \
            find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -not -name 'wildcards-*.txt' -not -name 'puredns-output-*.txt' -exec stat -c '%n %s bytes' {} +
          # show first/last 5 lines for large files to inspect unexpected content
          find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -size +1k -print0 | while IFS= read -r -d '' f; do
                    echo "== $f (head) =="
                    head -n5 "$f" || true
                    echo "== $f (tail) =="
                    tail -n5 "$f" || true
          done
          # repository-side view: what results/ files already exist in the cloned repo
          echo "DEBUG: repository results/ tree (in $TMP_DIR) before merge (ls -l):"
          ls -l "$TMP_DIR/results" 2>/dev/null || true
          git -C "$TMP_DIR" status --porcelain --untracked-files=no || true
          git -C "$TMP_DIR" ls-files -s results/** 2>/dev/null || true
          git add results/
          if git diff --staged --quiet; then
                    echo "No new unique data to commit for shard ${CHUNK_INDEX} after merging."
                    exit 0
          fi
          git commit -m "feat: Update results from shard ${CHUNK_INDEX}"
          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
                    echo "[Attempt $i/$MAX_ATTEMPTS] Pushing results for shard ${CHUNK_INDEX}..."
                    if git pull --rebase origin "$GIT_BRANCH" && git push origin "$GIT_BRANCH"; then
                              echo "✅ Successfully pushed results for shard ${CHUNK_INDEX}."
                              exit 0
                    fi
                    echo "Push failed. Retrying in $(( 5 * i )) seconds..."
                    sleep $(( 5 * i ))
          done
          echo "::error:: All push attempts failed for shard ${CHUNK_INDEX}."
          exit 0


      - name: Clone Fresh Repository
        shell: bash
        run: |
          echo "Cloning repository to get committed subdomain_ports files..."
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          
          # Move to parent directory
          cd ..
          REPO_NAME="${{ github.event.repository.name }}"
          
          # Remove if exists
          rm -rf "${REPO_NAME}-httpx"
          
          # Clone fresh copy
          git clone --branch ${{ github.ref_name }} --depth 1 https://github.com/${{ github.repository }}.git "${REPO_NAME}-httpx"
          
          # Verify subdomain_ports files exist
          if [ -d "${REPO_NAME}-httpx/results" ]; then
              FILE_COUNT=$(find "${REPO_NAME}-httpx/results" -type f -name "subdomain_ports-${CHUNK_INDEX}.txt" | wc -l)
              echo "✅ Found $FILE_COUNT subdomain_ports-${CHUNK_INDEX}.txt files"
          fi
          
          # Return to workspace
          cd "$REPO_NAME"
      
      
      - name: Prepare HTTPx Input and Domain Mapping
        shell: bash
        run: |
          
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          REPO_NAME="${{ github.event.repository.name }}"
          CLONED_RESULTS="../${REPO_NAME}-httpx/results"
          
          echo "Preparing HTTPx input from subdomain_ports-${CHUNK_INDEX}.txt files..."
          
          mkdir -p httpx-work
          
          HTTPX_INPUT="httpx-work/httpx-input-${CHUNK_INDEX}.txt"
          DOMAIN_MAP="httpx-work/domain-map-${CHUNK_INDEX}.txt"
          
          # Initialize files
          : > "$HTTPX_INPUT"
          : > "$DOMAIN_MAP"
          
          # Find all subdomain_ports files for this chunk across all domains
          find "$CLONED_RESULTS" -type f -name "subdomain_ports-${CHUNK_INDEX}.txt" | while read -r file; do
              if [ ! -s "$file" ]; then
                  continue
              fi
              
              # Extract root domain from path: ../repo/results/bmw.com/subdomain_ports-*.txt
              ROOT_DOMAIN=$(echo "$file" | sed "s|.*${CLONED_RESULTS}/\([^/]*\)/.*|\1|")
              
              echo "  -> Processing $ROOT_DOMAIN (chunk ${CHUNK_INDEX})"
              
              # Process each line from subdomain_ports file
              while IFS= read -r line || [ -n "$line" ]; do
                  [ -z "$line" ] && continue
                  
                  # Add to httpx input
                  echo "$line" >> "$HTTPX_INPUT"
                  
                  # Create mapping: target root_domain
                  echo "$line $ROOT_DOMAIN" >> "$DOMAIN_MAP"
              done < "$file"
          done
          
          # Deduplicate
          sort -u "$HTTPX_INPUT" -o "$HTTPX_INPUT" || true
          sort -u "$DOMAIN_MAP" -o "$DOMAIN_MAP" || true
          
          TARGET_COUNT=$(wc -l < "$HTTPX_INPUT" 2>/dev/null || echo 0)
          
          if [ "$TARGET_COUNT" -gt 0 ]; then
              echo "✅ Prepared $TARGET_COUNT targets for HTTPx scanning (shard ${{ matrix.shard }})"
          else
              echo "⚠️  No targets found for shard ${{ matrix.shard }}"
          fi
      
      - name: Run HTTPx Scan
        shell: bash
        run: |
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          HTTPX_INPUT="httpx-work/httpx-input-${CHUNK_INDEX}.txt"
          HTTPX_OUTPUT="httpx-work/httpx-output-${CHUNK_INDEX}.txt"

          if [ ! -s "$HTTPX_INPUT" ]; then
              echo "⚠️  No targets to scan for shard ${{ matrix.shard }}"
              exit 0
          fi

          TARGET_COUNT=$(wc -l < "$HTTPX_INPUT" 2>/dev/null || echo 0)
          echo "Running HTTPx on shard ${{ matrix.shard }} with $TARGET_COUNT targets..."

          # Run HTTPx with -l (list) flag for input and -o for output
          httpx \
            -l "$HTTPX_INPUT" \
            -status-code \
            -content-length \
            -title \
            -no-color \
            -threads 100 \
            -timeout 10 \
            -retries 1 \
            -rate-limit 150 \
            -o "$HTTPX_OUTPUT" \
            -silent

          if [ -f "$HTTPX_OUTPUT" ] && [ -s "$HTTPX_OUTPUT" ]; then
              RESULT_COUNT=$(wc -l < "$HTTPX_OUTPUT" 2>/dev/null || echo 0)
              echo "✅ HTTPx scan complete: $RESULT_COUNT live hosts found (shard ${{ matrix.shard }})"
          else
              echo "⚠️  No live hosts found (shard ${{ matrix.shard }})"
              : > "$HTTPX_OUTPUT"
          fi
      
      - name: Distribute HTTPx Results to Domain Directories
        shell: bash
        run: |
                    
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          HTTPX_OUTPUT="httpx-work/httpx-output-${CHUNK_INDEX}.txt"
          DOMAIN_MAP="httpx-work/domain-map-${CHUNK_INDEX}.txt"
          
          if [ ! -s "$HTTPX_OUTPUT" ]; then
              echo "No HTTPx results to distribute for shard ${{ matrix.shard }}"
              exit 0
          fi
          
          echo "Distributing HTTPx results to domain directories (shard ${{ matrix.shard }})..."
          
          # Create results directory structure
          mkdir -p results
          
          # Process each HTTPx result line
          PROCESSED_COUNT=0
          
          while IFS= read -r line || [ -n "$line" ]; do
              [ -z "$line" ] && continue
              
              # Extract URL (first field)
              URL=$(echo "$line" | awk '{print $1}')
              [ -z "$URL" ] && continue
              
              # Extract host:port from URL (remove protocol)
              HOST_PORT=$(echo "$URL" | sed -E 's|^https?://||')
              
              # Find root domain from mapping
              # Method 1: Exact match with port
              ROOT_DOMAIN=$(awk -v target="$HOST_PORT" '$1 == target {print $2; exit}' "$DOMAIN_MAP")
              
              # Method 2: Match without port
              if [ -z "$ROOT_DOMAIN" ]; then
                  HOST_ONLY=$(echo "$HOST_PORT" | cut -d: -f1)
                  ROOT_DOMAIN=$(awk -v target="$HOST_ONLY" '$1 == target {print $2; exit}' "$DOMAIN_MAP")
              fi
              
              # Method 3: Pattern match on subdomain
              if [ -z "$ROOT_DOMAIN" ]; then
                  HOST_ONLY=$(echo "$HOST_PORT" | cut -d: -f1)
                  ROOT_DOMAIN=$(awk -v host="$HOST_ONLY" '
                    {
                      domain = $2
                      if (host == domain || host ~ "\\." domain "$") {
                        print domain
                        exit
                      }
                    }
                  ' "$DOMAIN_MAP")
              fi
              
              if [ -n "$ROOT_DOMAIN" ]; then
                  # Create domain directory
                  mkdir -p "results/$ROOT_DOMAIN"
                  
                  # Append URL to domain's httpx file for this chunk
                  echo "$URL" >> "results/$ROOT_DOMAIN/httpx_results-${CHUNK_INDEX}.txt"
                  PROCESSED_COUNT=$((PROCESSED_COUNT + 1))
              fi
          done < "$HTTPX_OUTPUT"
          
          # Deduplicate each domain's httpx results file
          find results -type f -name "httpx_results-${CHUNK_INDEX}.txt" | while read -r file; do
              sort -u "$file" -o "$file"
          done
          
          echo "✅ Distributed $PROCESSED_COUNT URLs to domain directories (shard ${{ matrix.shard }})"

      - name: Commit HTTPx Results
        shell: bash
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
          GIT_REPO: ${{ github.repository }}
          GIT_BRANCH: ${{ github.ref_name }}
        run: |
          SOURCE_RESULTS_DIR="${GITHUB_WORKSPACE}/results"
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          TMP_DIR=$(mktemp -d)
          echo "Cloning ${GIT_REPO} into temporary directory ${TMP_DIR} for HTTPx results..."
          git clone --depth 1 "https://x-access-token:${PAT_TOKEN}@github.com/${GIT_REPO}.git" --branch "$GIT_BRANCH" "$TMP_DIR"
          cd "$TMP_DIR"
          git config user.name "GitHub Actions Bot"
          git config user.email "actions-bot@users.noreply.github.com"

          # This function is for merging the new data into the repository
          merge_new_data() {
              local source_file="$1"
              local relative_path="${source_file#$SOURCE_RESULTS_DIR/}"
              local dest_file="$TMP_DIR/results/$relative_path"
              mkdir -p "$(dirname "$dest_file")"

              # Append new results and then sort unique
              cat "$source_file" >> "$dest_file.tmp"
              sort -u "$dest_file.tmp" > "$dest_file"
              rm "$dest_file.tmp"
          }

          # Find and process only the httpx_results files
          while IFS= read -r -d $'\0' file; do
              echo "  -> Merging $file into repository's results/ directory"
              merge_new_data "$file"
          done < <(find "$SOURCE_RESULTS_DIR" -type f -name "httpx_results-${CHUNK_INDEX}.txt" -print0)

          git add results/
          if git diff --staged --quiet; then
              echo "No new unique httpx data to commit for shard ${CHUNK_INDEX}."
              exit 0
          fi

          git commit -m "feat: Add httpx results from shard ${CHUNK_INDEX}"

          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
              echo "[Attempt $i/$MAX_ATTEMPTS] Pushing httpx results for shard ${CHUNK_INDEX}..."
              if git pull --rebase origin "$GIT_BRANCH" && git push origin "$GIT_BRANCH"; then
                  echo "✅ Successfully pushed httpx results for shard ${CHUNK_INDEX}."
                  exit 0
              fi
              echo "Push failed. Retrying in $(( 5 * i )) seconds..."
              sleep $(( 5 * i ))
          done

          echo "::error:: All push attempts failed for httpx results from shard ${CHUNK_INDEX}."
          exit 0


      - name: Upload HTTPx Results for This Shard
        uses: actions/upload-artifact@v4
        with:
          name: httpx-results-${{ matrix.shard }}
          path: results/**/httpx_results-*.txt
          retention-days: 1          

  dispatch-next:
    name: dispatch-next-run
    runs-on: ubuntu-latest
    
    needs: [prepare, brute]
    if: always()
    env:
      PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
    steps:
      - name: Compute and dispatch next run
        shell: bash
        run: |
          if [ -z "${PAT_TOKEN}" ]; then echo "Error: secrets.PAT_TOKEN is required." >&2; exit 0; fi
          CHUNK_START=${{ github.event.inputs.chunk_start }}
          TOTAL_CHUNKS=${{ needs.prepare.outputs.total-chunks }}
          RUN_COUNTER=${{ github.event.inputs.run_counter }}
          MAX_RUNS=${{ needs.prepare.outputs.effective-max-runs }}
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          NEXT_START=$((CHUNK_START + MATRIX_SIZE))
          NEXT_COUNTER=$((RUN_COUNTER + 1))
          echo "Current start: ${CHUNK_START}, Next start: ${NEXT_START}, Total chunks: ${TOTAL_CHUNKS}"
          if [ "$NEXT_START" -ge "$TOTAL_CHUNKS" ]; then echo "All chunks processed. Terminating."; exit 0; fi
          if [ "${NEXT_COUNTER}" -ge "${MAX_RUNS}" ]; then echo "Next run would exceed max_runs limit. Terminating."; exit 0; fi
          URL="https://api.github.com/repos/${{ github.repository }}/actions/workflows/${{ env.WORKFLOW_FILE }}/dispatches"
          BODY=$(printf '{"ref":"%s","inputs":{"chunk_start":"%s","total_chunks":"%s","run_counter":"%s","max_runs":"%s","words_per_chunk":"%s","domains_count":"%s"}}' \
            "${{ github.ref_name }}" "${NEXT_START}" "${TOTAL_CHUNKS}" "${NEXT_COUNTER}" "${MAX_RUNS}" \
            "${{ github.event.inputs.words_per_chunk }}" "${{ github.event.inputs.domains_count }}")
          echo "Dispatching next run..."
          for i in 1 2 3 4; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" -X POST -H "Accept: application/vnd.github+json" \
              -H "Authorization: Bearer ${PAT_TOKEN}" -H "X-GitHub-Api-Version: 2022-11-28" -d "$BODY" "$URL")
            if [ "$HTTP_STATUS" -ge 200 ] && [ "$HTTP_STATUS" -lt 300 ]; then echo "Dispatch succeeded (HTTP ${HTTP_STATUS})"; exit 0; fi
            echo "Dispatch attempt ${i} failed (HTTP ${HTTP_STATUS}). Retrying..."
            sleep $((i * 2))
          done
          echo "Failed to dispatch next run after retries. Terminating with an error."
          exit 0
