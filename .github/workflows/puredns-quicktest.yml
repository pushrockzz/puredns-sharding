name: puredns-quick-test

on:
  workflow_dispatch:
    inputs:
      chunk_start:
        description: 'First chunk index this run will process (integer)'
        required: true
        default: '0'
      total_chunks:
        description: 'DEPRECATED: Total chunks is now auto-detected.'
        required: false
      run_counter:
        description: 'Automatic run counter for recursion (auto-incremented)'
        required: false
        default: '0'
      max_runs:
        description: 'Safety cap: stop after this many runs. If unset, it is auto-calculated.'
        required: false
        default: '999'
      words_per_chunk:
        description: 'Words per chunk file (lines_per_chunk).'
        required: false
        default: '40000'
      domains_count:
        description: 'How many test domains to generate (default = 5)'
        required: false
        default: '5'

env:
  WORKFLOW_FILE: puredns-quicktest.yml
  MATRIX_SIZE: '20'

jobs:
  prepare:
    name: prepare-wordlists-and-resolvers
    runs-on: ubuntu-latest
    env:
      COMBINED_WORDLIST: combined-wordlist.txt
    outputs:
      artifact-name: wordlists-artifact
      total-chunks: ${{ steps.finalize_prep.outputs.total-chunks }}
      effective-max-runs: ${{ steps.finalize_prep.outputs.effective-max-runs }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: false
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'
      - name: Install Go tools
        run: go install -v github.com/tomnomnom/anew@latest
      - name: Cache Raw Wordlists
        uses: actions/cache@v3
        with:
          path: |
            best-wordlist-level1.txt
            best-wordlist-level2.txt
          key: raw-wordlists-cache-v1-${{ runner.os }}
      - name: Fetch wordlists and Resolvers
        run: |
          if [ ! -f best-wordlist-level1.txt ]; then wget -qO best-wordlist-level1.txt https://raw.githubusercontent.com/trickest/wordlists/main/inventory/levels/level1.txt; fi
          if [ ! -f best-wordlist-level2.txt ]; then wget -qO best-wordlist-level2.txt https://raw.githubusercontent.com/trickest/wordlists/main/inventory/levels/level2.txt; fi
          wget -qO resolvers.txt https://raw.githubusercontent.com/rix4uni/resolvers/main/resolvers.txt
          wget -qO resolvers-trusted.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/main/resolvers.txt
      - name: Build filter_wordlist tool
        run: |
          if [ -f "filter_wordlist.go" ]; then go build -o filter_wordlist filter_wordlist.go; fi

      - name: Cache Filtered Wordlist
        id: cache-filtered-wordlist
        uses: actions/cache@v3
        with:
          path: ${{ env.COMBINED_WORDLIST }}
          key: filtered-wordlist-cache-v1-${{ runner.os }}-${{ hashFiles('best-wordlist-*.txt', 'filter_wordlist.go') }}
      - name: Filter and Combine Wordlists
        if: steps.cache-filtered-wordlist.outputs.cache-hit != 'true'
        run: |
          if [ -f ./filter_wordlist ]; then
            ./filter_wordlist best-wordlist-level1.txt > best-wordlist-filtered.txt
            ./filter_wordlist best-wordlist-level2.txt > best-wordlist-level2-filtered.txt
          else
            cp best-wordlist-level1.txt best-wordlist-filtered.txt
            cp best-wordlist-level2.txt best-wordlist-level2-filtered.txt
          fi
          cat best-wordlist-filtered.txt best-wordlist-level2-filtered.txt | PATH=$HOME/go/bin:$PATH anew -q ${{ env.COMBINED_WORDLIST }}
      - name: Finalize Preparation and Split Chunks
        id: finalize_prep
        run: |
          mkdir -p domains wordlists results
          if [ ! -f "domains/domains.txt" ]; then
            for i in $(seq 1 "${{ github.event.inputs.domains_count }}"); do echo "test${i}.example.com"; done > "domains/domains.txt"
          fi
          COMBINED_WORDLIST="${{ env.COMBINED_WORDLIST }}"
          if [ ! -s "$COMBINED_WORDLIST" ]; then echo "::error:: Combined wordlist is empty."; exit 0; fi
          rm -f wordlists/chunk-*.txt || true
          split -l "${{ github.event.inputs.words_per_chunk }}" -d -a 5 --additional-suffix=.txt "$COMBINED_WORDLIST" wordlists/chunk-
          if [ -z "$(ls -A wordlists)" ]; then echo "::error:: Split failed to create chunks."; exit 0; fi
          CHUNK_COUNT=$(ls -1 wordlists/chunk-*.txt | wc -l)
          echo "total-chunks=${CHUNK_COUNT}" >> $GITHUB_OUTPUT
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          CALCULATED_RUNS=$(( (CHUNK_COUNT + MATRIX_SIZE - 1) / MATRIX_SIZE ))
          USER_MAX_RUNS=${{ github.event.inputs.max_runs }}
          EFFECTIVE_MAX_RUNS=$(( CALCULATED_RUNS < USER_MAX_RUNS ? CALCULATED_RUNS : USER_MAX_RUNS ))
          echo "effective-max-runs=${EFFECTIVE_MAX_RUNS}" >> $GITHUB_OUTPUT
      
      - name: Upload binaries and wordlists as artifact
        uses: actions/upload-artifact@v4
        with:
          name: wordlists-artifact
          path: |
            wordlists
            resolvers.txt
            resolvers-trusted.txt
            domains/domains.txt


  generate_matrix:
    name: Generate Dynamic Matrix
    runs-on: ubuntu-latest
    needs: prepare
    outputs:
      shard_matrix: ${{ steps.gen_matrix.outputs.shard_matrix }}
    steps:
      - name: Calculate shards for this run
        id: gen_matrix
        run: |
          TOTAL_CHUNKS=${{ needs.prepare.outputs.total-chunks }}
          CHUNK_START=${{ github.event.inputs.chunk_start }}
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          CHUNKS_REMAINING=$(( TOTAL_CHUNKS - CHUNK_START ))
          if (( CHUNKS_REMAINING < 0 )); then CHUNKS_REMAINING=0; fi
          SHARDS_THIS_RUN=$(( CHUNKS_REMAINING < MATRIX_SIZE ? CHUNKS_REMAINING : MATRIX_SIZE ))
          if (( SHARDS_THIS_RUN <= 0 )); then
            echo "shard_matrix=[]" >> $GITHUB_OUTPUT
          else
            MATRIX_JSON=$(jq -cn --argjson n "$SHARDS_THIS_RUN" '[range($n)]')
            echo "Generated matrix for ${SHARDS_THIS_RUN} shards: $MATRIX_JSON"
            echo "shard_matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT
          fi

  brute:
    name: bruteforce-shard (shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    needs: [prepare, generate_matrix]
    if: needs.generate_matrix.outputs.shard_matrix != '[]'
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      fail-fast: false
      matrix:
        shard: ${{ fromJson(needs.generate_matrix.outputs.shard_matrix) }}
    steps:
      - name: Checkout repo (for committing results)
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0   
          
      - name: Download artifacts (binaries and wordlists)
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.prepare.outputs.artifact-name }}
          path: .

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

      - name: Install Tools
        run: |

          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi    
          
      - name: Run puredns bruteforce
        shell: bash
        run: |
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          CHUNK_FILE="wordlists/chunk-$(printf '%05d' "$CHUNK_INDEX").txt"
          if [ ! -f "$CHUNK_FILE" ]; then exit 0; fi
          while IFS= read -r domain || [ -n "$domain" ]; do
            case "$domain" in \#*|"") continue ;; esac
            ROOT_DOMAIN_DIR="results/$domain"
            mkdir -p "$ROOT_DOMAIN_DIR"
            cat "$CHUNK_FILE" | puredns bruteforce "$domain" -r "resolvers.txt" --rate-limit 5000 --rate-limit-trusted 2000 \
              --resolvers-trusted "resolvers-trusted.txt" --wildcard-tests 300 --wildcard-batch 100000 \
              --write "${ROOT_DOMAIN_DIR}/puredns-output-${CHUNK_INDEX}.txt" \
              --write-wildcards "${ROOT_DOMAIN_DIR}/wildcards-${CHUNK_INDEX}.txt" \
              --write-massdns "${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt" --quiet


            PUREDNS_FILE="${ROOT_DOMAIN_DIR}/puredns-output-${CHUNK_INDEX}.txt"
            MASSDNS_FILE="${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt"
          
            # per-domain tmpdir for isolation
            SAFE_DOMAIN_NAME=$(echo "$domain" | sed 's/[^A-Za-z0-9._-]/_/g')
            DOMAIN_TMPDIR=$(mktemp -d -p "$(pwd)" "tmp-${SAFE_DOMAIN_NAME}-XXXX")
            TMP_PURE="$DOMAIN_TMPDIR/puredns-valid.txt"
            TMP_CLEAN="$DOMAIN_TMPDIR/massdns-clean.txt"
            TMP_FILTER="$DOMAIN_TMPDIR/massdns-filtered.txt"
          
            # Build normalized PureDNS host list (strip trailing dot, lowercase, unique) if present
            if [ -s "$PUREDNS_FILE" ]; then
              awk '{ gsub(/\.$/,"",$1); print tolower($1) }' "$PUREDNS_FILE" | sort -u > "$TMP_PURE"
              PURE_COUNT=$(wc -l < "$TMP_PURE" 2>/dev/null || echo 0)
            else
              PURE_COUNT=0
              : > "$TMP_PURE"
            fi
          
            # If massdns exists, clean it into canonical lines "host A ip"
            if [ -s "$MASSDNS_FILE" ]; then
              awk 'NF {
                # normalize host (first field), find first "A" token (any field) and take next field as IP
                host=$1; sub(/\.$/,"",host);
                aidx=0
                for(i=2;i<=NF;i++){
                  if($i=="A"){ aidx=i; break }
                }
                if(aidx>0 && (aidx+1)<=NF){
                  ip=$(aidx+1);
                  gsub(/^[ \t]+|[ \t]+$/,"",host);
                  gsub(/^[ \t]+|[ \t]+$/,"",ip);
                  if(host!="" && ip!="") print host " A " ip;
                }
              }' "$MASSDNS_FILE" | sort -u > "$TMP_CLEAN" || true
            else
              # no massdns produced (create empty clean file)
              : > "$TMP_CLEAN"
            fi
          
            # Filter cleaned massdns against puredns hostlist if puredns exists; otherwise keep cleaned entries
            if [ -s "$TMP_PURE" ]; then
              awk -v validfile="$TMP_PURE" '
                BEGIN {
                  while((getline v < validfile) > 0) valid[tolower(v)] = 1
                  close(validfile)
                }
                NF>=3 {
                  host=$1; host_l=tolower(host);
                  if(host_l in valid) print $0
                }
              ' "$TMP_CLEAN" > "$TMP_FILTER" || true
            else
              cp "$TMP_CLEAN" "$TMP_FILTER" || true
            fi
          
            # Atomic replace: move filtered output back to domain massdns file
            mv "$TMP_FILTER" "$MASSDNS_FILE"
            rm -f "$TMP_CLEAN"
          
            # Count massdns lines for echo (safe even if empty)
            MASSDNS_COUNT=$(wc -l < "$MASSDNS_FILE" 2>/dev/null || echo 0)
          
            # Echo exact messages you requested
            echo "✅ PureDNS resolution complete. ${PURE_COUNT} subdomains were successfully resolved."
            echo "✅ PureDNS resolution complete. ${MASSDNS_COUNT} massdns found."
          
            # remove per-domain tmpdir
            rm -rf "$DOMAIN_TMPDIR"
          
          done < "domains/domains.txt"

      - name: Perform Port Scanning
        shell: bash
        run: |
          # Disable exit on error to handle failures gracefully
         
          
          PORTS="80,443,8080,8443,8000"
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          
          # Create a single, global temporary directory for this entire step's operations
          GLOBAL_TMPDIR=$(mktemp -d)
          # This 'trap' command ensures the temporary directory is automatically deleted when the step exits
          trap 'rm -rf "$GLOBAL_TMPDIR"' EXIT

          # --- PHASE 1: AGGREGATE ALL TARGETS ---
          echo "PHASE 1: Aggregating all targets from all domains..."
          
          # Define paths for our new master files inside the safe temporary directory
          MASTER_IP_TO_SUBDOMAIN_MAP="${GLOBAL_TMPDIR}/master_ip_to_subdomain.txt"
          MASTER_UNIQUE_IPS="${GLOBAL_TMPDIR}/master_unique_ips.txt"

          # Initialize empty files
          : > "$MASTER_IP_TO_SUBDOMAIN_MAP"

          # Loop through domains for the sole purpose of collecting IPs and mapping them.
          while IFS= read -r domain || [ -n "$domain" ]; do
              case "$domain" in \#*|"") continue ;; esac
              ROOT_DOMAIN_DIR="results/$domain"
              MASSDNS_INPUT_FILE="${ROOT_DOMAIN_DIR}/massdns-${CHUNK_INDEX}.txt"

              if [ ! -s "$MASSDNS_INPUT_FILE" ]; then
                  continue
              fi

              # Read the massdns file, clean it, and create a map of "IP SUBDOMAIN".
              # Append this to our master map file. This preserves the crucial link.
              awk 'NF>=3 && $2=="A" { host=$1; sub(/\.$/,"",host); print $3, host }' "$MASSDNS_INPUT_FILE" \
                | sort -u >> "$MASTER_IP_TO_SUBDOMAIN_MAP" || true
          done < "domains/domains.txt"

          if [ ! -s "$MASTER_IP_TO_SUBDOMAIN_MAP" ]; then
              echo "⚠️  No IPs found across any domains. Exiting port scanning step."
              exit 0
          fi

          # Create the master list of unique IPs to be scanned from our map.
          cut -d' ' -f1 "$MASTER_IP_TO_SUBDOMAIN_MAP" | sort -u > "$MASTER_UNIQUE_IPS" || true
          echo "✅ Phase 1 Complete: Collected $(wc -l < "$MASTER_UNIQUE_IPS" 2>/dev/null || echo 0) unique IPs from $(wc -l < "$MASTER_IP_TO_SUBDOMAIN_MAP" 2>/dev/null || echo 0) total mappings."

          # --- PHASE 2: PERFORM A SINGLE, CONSOLIDATED SCAN ---
          echo "PHASE 2: Performing one consolidated scan..."

          MASTER_NON_CDN_IPS="${GLOBAL_TMPDIR}/master_non_cdn.txt"
          MASTER_SCAN_RESULTS="${GLOBAL_TMPDIR}/master_scan_results.txt"
          
          # Initialize empty file
          : > "$MASTER_NON_CDN_IPS"
          
          # Run CDN check ONCE on the master IP list
          cat "$MASTER_UNIQUE_IPS" | cut-cdn -ua -t 50 -silent -o "$MASTER_NON_CDN_IPS" || true

          # If any non-CDN IPs exist, run the scanners ONCE
          if [ -s "$MASTER_NON_CDN_IPS" ]; then
              echo "  -> Scanning $(wc -l < "$MASTER_NON_CDN_IPS" 2>/dev/null || echo 0) non-CDN IPs..."
              TMP_NAABU_RUSTSCAN=$(mktemp)
              TMP_RUSTSCAN=$(mktemp)
              
              # Your original tool commands, unchanged
              naabu -l "$MASTER_NON_CDN_IPS" -passive -o "$TMP_NAABU_RUSTSCAN" -no-color -silent || true
              rustscan -a "$MASTER_NON_CDN_IPS" -p "$PORTS" --no-banner -t 5000 -b 300 --greppable > "$TMP_RUSTSCAN" || true
              
              # Your original processing logic, unchanged
              cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }' | PATH=$HOME/go/bin:$PATH anew -q "$TMP_NAABU_RUSTSCAN" || true
              
              # Process scan results into consistent format: "IP PORT"
              awk -F: 'NF==2 { print $1, $2 } NF==1 { print $1, "NOPORT" }' "$TMP_NAABU_RUSTSCAN" | sort -u > "$MASTER_SCAN_RESULTS" || true
              rm -f "$TMP_RUSTSCAN" "$TMP_NAABU_RUSTSCAN" 2>/dev/null || true
          else
              # Create empty scan results file
              : > "$MASTER_SCAN_RESULTS"
          fi
          
          # Add CDN IPs back to the results list with NOPORT marker for consistency
          MASTER_CDN_IPS="${GLOBAL_TMPDIR}/master_cdn_ips.txt"
          comm -13 <(sort "$MASTER_NON_CDN_IPS" 2>/dev/null || true) <(sort "$MASTER_UNIQUE_IPS" 2>/dev/null || true) > "$MASTER_CDN_IPS" 2>/dev/null || true
          if [ -s "$MASTER_CDN_IPS" ]; then
              awk '{ print $1, "NOPORT" }' "$MASTER_CDN_IPS" >> "$MASTER_SCAN_RESULTS" || true
          fi
          
          echo "✅ Phase 2 Complete: Scan finished ($(wc -l < "$MASTER_SCAN_RESULTS" 2>/dev/null || echo 0) total IP-port combinations)."

          # --- PHASE 3: DISTRIBUTE RESULTS ---
          echo "PHASE 3: Distributing results to domain directories..."
          
          # Sort master files once for efficient processing
          MASTER_SCAN_RESULTS_SORTED="${GLOBAL_TMPDIR}/master_scan_sorted.txt"
          MASTER_MAP_SORTED="${GLOBAL_TMPDIR}/master_map_sorted.txt"
          sort -k1,1 "$MASTER_SCAN_RESULTS" > "$MASTER_SCAN_RESULTS_SORTED" 2>/dev/null || true
          sort -k1,1 "$MASTER_IP_TO_SUBDOMAIN_MAP" > "$MASTER_MAP_SORTED" 2>/dev/null || true
          
          # Verify sorted files exist
          if [ ! -f "$MASTER_SCAN_RESULTS_SORTED" ] || [ ! -f "$MASTER_MAP_SORTED" ]; then
              echo "⚠️  Error: Failed to create sorted master files. Exiting Phase 3."
              exit 0
          fi
          
          # Loop through domains a final time to generate their specific report files
          DOMAINS_PROCESSED=0
          DOMAINS_SKIPPED=0
          
          while IFS= read -r domain || [ -n "$domain" ]; do
              case "$domain" in \#*|"") continue ;; esac
              ROOT_DOMAIN_DIR="results/$domain"
              OUTPUT_FILE="${ROOT_DOMAIN_DIR}/subdomain_ports-${CHUNK_INDEX}.txt"
              
              # Escape domain for safe regex matching - only escape dots for regex
              ESCAPED_DOMAIN=$(printf '%s\n' "$domain" | sed 's/\./\\./g' 2>/dev/null || echo "$domain")
              
              # Extract only the map entries for this specific domain
              # Pattern: matches any line where second field ends with .domain.com OR is exactly domain.com
              # The map format is: "IP SUBDOMAIN", so we match on the SUBDOMAIN field
              DOMAIN_MAP_TMP=$(mktemp)
              awk -v domain="$domain" '
                $2 == domain || $2 ~ "\\." domain "$" { print $0 }
              ' "$MASTER_MAP_SORTED" > "$DOMAIN_MAP_TMP" 2>/dev/null || true

              if [ ! -s "$DOMAIN_MAP_TMP" ]; then
                  echo "  -> ⚠️  Skipping $domain (no IPs found in master map)"
                  DOMAINS_SKIPPED=$((DOMAINS_SKIPPED + 1)) || true
                  rm -f "$DOMAIN_MAP_TMP" 2>/dev/null || true
                  continue
              fi
              
              # Join scan results with this domain's IP-subdomain map
              # Output format after join: IP PORT SUBDOMAIN
              join -t' ' -1 1 -2 1 "$MASTER_SCAN_RESULTS_SORTED" "$DOMAIN_MAP_TMP" 2>/dev/null | \
                awk '{
                    ip = $1
                    port = $2
                    subdomain = $3
                    
                    # If port exists and is not NOPORT marker, format as subdomain:port
                    if (port != "NOPORT" && port ~ /^[0-9]+$/) {
                        print subdomain ":" port
                    } else {
                        # No port data (CDN IP), just print subdomain
                        print subdomain
                    }
                  }' 2>/dev/null | \
                sort -u > "$OUTPUT_FILE" 2>/dev/null || true

              if [ -s "$OUTPUT_FILE" ]; then
                  LINE_COUNT=$(wc -l < "$OUTPUT_FILE" 2>/dev/null || echo 0)
                  echo "  -> ✅ Generated $OUTPUT_FILE for $domain ($LINE_COUNT entries)"
                  DOMAINS_PROCESSED=$((DOMAINS_PROCESSED + 1)) || true
              else
                  echo "  -> ⚠️  Warning: $domain has no port scan results (empty output)"
                  DOMAINS_SKIPPED=$((DOMAINS_SKIPPED + 1)) || true
              fi
              
              rm -f "$DOMAIN_MAP_TMP" 2>/dev/null || true
          done < "domains/domains.txt"
          
          echo "✅ Phase 3 Complete: $DOMAINS_PROCESSED domains processed, $DOMAINS_SKIPPED domains skipped."
          
          # Explicitly exit with success
          exit 0
          
      - name: Sanitize and Finalize Results
        shell: bash
        run: |
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          echo "DEBUG: matrix.shard='${{ matrix.shard }}' chunk_start='${{ github.event.inputs.chunk_start }}' CHUNK_INDEX=$CHUNK_INDEX"
          
          # show any pre-existing result files for this chunk (if any)
          find "results" -type f -name "*-${CHUNK_INDEX}.txt" -printf "%p %s bytes\n" 2>/dev/null || \
            find "results" -type f -name "*-${CHUNK_INDEX}.txt" -exec stat -c '%n %s bytes' {} + 2>/dev/null || true
          
          echo "Sanitizing results for shard ${CHUNK_INDEX}..."
          
          find "results" -type f -name "*-${CHUNK_INDEX}.txt" 2>/dev/null | while read file; do
            echo "  -> Processing $file"
            TEMP_FILE=$(mktemp)
            
            if [[ "$file" == *"massdns-"* ]]; then
              awk 'NF > 0 { $1=$1; print }' "$file" > "$TEMP_FILE"
            else
              awk 'NF > 0 { sub(/^[ \t]+/, ""); sub(/[ \t]+$/, ""); print }' "$file" > "$TEMP_FILE"
            fi
            
            mv "$TEMP_FILE" "$file"
          done
          
          echo "✅ Sanitization complete for shard ${CHUNK_INDEX}."     

      - name: Commit Shard-Specific Results
        shell: bash
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
          GIT_REPO: ${{ github.repository }}
          GIT_BRANCH: ${{ github.ref_name }}
        run: |
          SOURCE_RESULTS_DIR="${GITHUB_WORKSPACE}/results"
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))



          TMP_DIR=$(mktemp -d)
          echo "Cloning ${GIT_REPO} into temporary directory ${TMP_DIR}..."
          git clone --depth 1 "https://x-access-token:${PAT_TOKEN}@github.com/${GIT_REPO}.git" --branch "$GIT_BRANCH" "$TMP_DIR"
          cd "$TMP_DIR"

          git config user.name "GitHub Actions Bot"
          git config user.email "actions-bot@users.noreply.github.com"

          find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -printf "%p %s bytes\n"

          merge_new_data() {
                    local source_file="$1"
                    local relative_path="${source_file#$SOURCE_RESULTS_DIR/}"
                    local dest_file="$TMP_DIR/results/$relative_path"

                    mkdir -p "$(dirname "$dest_file")"
                    TEMP_MERGED=$(mktemp)

                    if [[ "$source_file" == *"massdns-"* ]]; then
                              # For massdns: we expect the source to be already filtered (see bruteforce step).
                              # Overwrite dest with the cleaned source (dedupe defensively).
                              awk '!seen[$0]++' "$source_file" > "$TEMP_MERGED"
                    else
                              # For other result types, sort-unique the source and write it as authoritative.
                              sort -u "$source_file" > "$TEMP_MERGED"
                    fi

                    mv "$TEMP_MERGED" "$dest_file"
          }

          while IFS= read -r -d $'\0' file; do
                    echo "  -> Merging $file into repository's results/ directory"
                    merge_new_data "$file"
          done < <(find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -print0)

          echo "DEBUG: Files found under SOURCE_RESULTS_DIR before merge:"
          find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -printf "%p %s bytes\n" || \
            find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -exec stat -c '%n %s bytes' {} +

          # show first/last 5 lines for large files to inspect unexpected content
          find "$SOURCE_RESULTS_DIR" -type f -name "*-${CHUNK_INDEX}.txt" -size +1k -print0 | while IFS= read -r -d '' f; do
                    echo "== $f (head) =="
                    head -n5 "$f" || true
                    echo "== $f (tail) =="
                    tail -n5 "$f" || true
          done

          # repository-side view: what results/ files already exist in the cloned repo
          echo "DEBUG: repository results/ tree (in $TMP_DIR) before merge (ls -l):"
          ls -l "$TMP_DIR/results" 2>/dev/null || true
          git -C "$TMP_DIR" status --porcelain --untracked-files=no || true
          git -C "$TMP_DIR" ls-files -s results/** 2>/dev/null || true

          git add results/

          if git diff --staged --quiet; then
                    echo "No new unique data to commit for shard ${CHUNK_INDEX} after merging."
                    exit 0
          fi

          git commit -m "feat: Update results from shard ${CHUNK_INDEX}"

          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
                    echo "[Attempt $i/$MAX_ATTEMPTS] Pushing results for shard ${CHUNK_INDEX}..."
                    if git pull --rebase origin "$GIT_BRANCH" && git push origin "$GIT_BRANCH"; then
                              echo "✅ Successfully pushed results for shard ${CHUNK_INDEX}."
                              exit 0
                    fi
                    echo "Push failed. Retrying in $(( 5 * i )) seconds..."
                    sleep $(( 5 * i ))
          done

          echo "::error:: All push attempts failed for shard ${CHUNK_INDEX}."
          exit 0

      - name: Prepare HTTPx Input from Subdomain Ports
        shell: bash
        run: |
         
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          echo "Preparing HTTPx input for chunk ${CHUNK_INDEX}..."
          
          # Create httpx working directory
          mkdir -p httpx-work
          
          HTTPX_INPUT="httpx-work/httpx-input-${CHUNK_INDEX}.txt"
          HTTPX_DOMAIN_MAP="httpx-work/httpx-domain-map-${CHUNK_INDEX}.txt"
          
          # Initialize files
          : > "$HTTPX_INPUT"
          : > "$HTTPX_DOMAIN_MAP"
          
          # Collect all targets from subdomain_ports files for THIS chunk
          find results -type f -name "subdomain_ports-${CHUNK_INDEX}.txt" | while read -r file; do
              if [ ! -s "$file" ]; then
                  continue
              fi
              
              # Extract root domain from path: results/bmw.com/subdomain_ports-*.txt
              ROOT_DOMAIN=$(echo "$file" | sed 's|results/\([^/]*\)/.*|\1|')
              
              # Read each line and prepare for httpx
              while IFS= read -r line || [ -n "$line" ]; do
                  # Skip empty lines
                  [ -z "$line" ] && continue
                  
                  # Add to httpx input (httpx will handle both formats)
                  echo "$line" >> "$HTTPX_INPUT"
                  
                  # Create mapping: "target -> root_domain" for later distribution
                  echo "$line $ROOT_DOMAIN" >> "$HTTPX_DOMAIN_MAP"
              done < "$file"
          done
          
          # Deduplicate
          sort -u "$HTTPX_INPUT" -o "$HTTPX_INPUT" || true
          sort -u "$HTTPX_DOMAIN_MAP" -o "$HTTPX_DOMAIN_MAP" || true
          
          TOTAL_TARGETS=$(wc -l < "$HTTPX_INPUT" 2>/dev/null || echo 0)
          
          if [ "$TOTAL_TARGETS" -gt 0 ]; then
              echo "✅ Prepared $TOTAL_TARGETS unique targets for HTTPx scanning"
          else
              echo "⚠️  No targets found for HTTPx scanning in chunk ${CHUNK_INDEX}"
          fi
      
      - name: Run HTTPx Scan
        shell: bash
        run: |
          
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          HTTPX_INPUT="httpx-work/httpx-input-${CHUNK_INDEX}.txt"
          HTTPX_RAW_OUTPUT="httpx-work/httpx-raw-${CHUNK_INDEX}.txt"
          
          if [ ! -s "$HTTPX_INPUT" ]; then
              echo "⚠️  No HTTPx input for chunk ${CHUNK_INDEX}, skipping scan"
              exit 0
          fi
          
          echo "Running HTTPx on chunk ${CHUNK_INDEX}..."
          echo "Scanning $(wc -l < "$HTTPX_INPUT") targets..."
          
          # Run httpx with comprehensive flags
          cat "$HTTPX_INPUT" | httpx \
            -silent \
            -no-color \
            -status-code \
            -content-length \
            -title \
            
            
            -threads 50 \
            -timeout 10 \
            -retries 2 \
            -rate-limit 150 \
            -o "$HTTPX_RAW_OUTPUT" 2>/dev/null || true
          
          if [ -s "$HTTPX_RAW_OUTPUT" ]; then
              RESULT_COUNT=$(wc -l < "$HTTPX_RAW_OUTPUT")
              echo "✅ HTTPx scan complete: $RESULT_COUNT live hosts found"
          else
              echo "⚠️  No live hosts found in this chunk"
              : > "$HTTPX_RAW_OUTPUT"
          fi
      
      - name: Process and Distribute HTTPx Results
        shell: bash
        run: |
          
          
          CHUNK_INDEX=$(( ${{ github.event.inputs.chunk_start }} + ${{ matrix.shard }} ))
          HTTPX_RAW_OUTPUT="httpx-work/httpx-raw-${CHUNK_INDEX}.txt"
          HTTPX_DOMAIN_MAP="httpx-work/httpx-domain-map-${CHUNK_INDEX}.txt"
          
          if [ ! -s "$HTTPX_RAW_OUTPUT" ]; then
              echo "No HTTPx results to process for chunk ${CHUNK_INDEX}"
              exit 0
          fi
          
          echo "Processing and distributing HTTPx results for chunk ${CHUNK_INDEX}..."
          
          # Create temporary mapping file
          TEMP_URL_DOMAIN_MAP=$(mktemp)
          
          # Process each HTTPx result line
          while IFS= read -r line || [ -n "$line" ]; do
              [ -z "$line" ] && continue
              
              # Extract the URL (first field before any brackets)
              URL=$(echo "$line" | awk '{print $1}')
              
              # Skip if URL is empty
              [ -z "$URL" ] && continue
              
              # Extract host:port from URL (remove http:// or https://)
              HOST_PORT=$(echo "$URL" | sed -E 's|^https?://||')
              
              # Try to find root domain from our mapping
              # Method 1: Exact match with port
              ROOT_DOMAIN=$(awk -v target="$HOST_PORT" '$1 == target {print $2; exit}' "$HTTPX_DOMAIN_MAP")
              
              # Method 2: Match without port
              if [ -z "$ROOT_DOMAIN" ]; then
                  HOST_ONLY=$(echo "$HOST_PORT" | cut -d: -f1)
                  ROOT_DOMAIN=$(awk -v target="$HOST_ONLY" '$1 == target {print $2; exit}' "$HTTPX_DOMAIN_MAP")
              fi
              
              # Method 3: Pattern match - find any line where target contains the host
              if [ -z "$ROOT_DOMAIN" ]; then
                  HOST_ONLY=$(echo "$HOST_PORT" | cut -d: -f1)
                  ROOT_DOMAIN=$(awk -v host="$HOST_ONLY" '$1 ~ host {print $2; exit}' "$HTTPX_DOMAIN_MAP")
              fi
              
              if [ -n "$ROOT_DOMAIN" ]; then
                  echo "$URL|$ROOT_DOMAIN" >> "$TEMP_URL_DOMAIN_MAP"
              else
                  echo "⚠️  Could not map URL to domain: $URL"
              fi
          done < "$HTTPX_RAW_OUTPUT"
          
          if [ ! -s "$TEMP_URL_DOMAIN_MAP" ]; then
              echo "⚠️  No URLs could be mapped to domains"
              rm -f "$TEMP_URL_DOMAIN_MAP"
              exit 0
          fi
          
          # Distribute results to domain directories
          DOMAINS_UPDATED=0
          
          # Get unique domains from mapping
          cut -d'|' -f2 "$TEMP_URL_DOMAIN_MAP" | sort -u | while read -r domain; do
              [ -z "$domain" ] && continue
              
              ROOT_DOMAIN_DIR="results/$domain"
              mkdir -p "$ROOT_DOMAIN_DIR"
              
              # Create per-chunk httpx result file
              HTTPX_RESULT_FILE="${ROOT_DOMAIN_DIR}/httpx_results-${CHUNK_INDEX}.txt"
              
              # Extract all URLs for this domain and write to file
              grep "|${domain}\$" "$TEMP_URL_DOMAIN_MAP" | cut -d'|' -f1 | sort -u > "$HTTPX_RESULT_FILE" || true
              
              if [ -s "$HTTPX_RESULT_FILE" ]; then
                  RESULT_COUNT=$(wc -l < "$HTTPX_RESULT_FILE")
                  echo "  -> ✅ Written $RESULT_COUNT HTTPx results to $HTTPX_RESULT_FILE"
                  DOMAINS_UPDATED=$((DOMAINS_UPDATED + 1))
              fi
          done
          
          rm -f "$TEMP_URL_DOMAIN_MAP"
          echo "✅ Distributed HTTPx results to $DOMAINS_UPDATED domain directories"
          
          # Clean up httpx working files
          rm -rf httpx-work
      
      # ============================================================================
      # THE COMMIT STEP (modified to include httpx results)
      # This replaces or modifies your existing commit step
      # ============================================================================
      
      - name: Upload Results (Modified for HTTPx)
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.shard }}
          path: results/
          retention-days: 1

  # ============================================================================
  # EXISTING COMMIT JOB (Modified)
  # ============================================================================
  commit:
    name: Commit All Results
    runs-on: ubuntu-latest
    needs: [brute]  # Replace with your actual job name
    if: always()
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Download All Result Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: downloaded-results/
          merge-multiple: true
      
      - name: Merge All Results by Domain
        shell: bash
        run: |
          echo "Merging all results from matrix shards..."
          
          # Process each domain directory
          find downloaded-results -mindepth 1 -maxdepth 1 -type d | while read -r domain_dir; do
              DOMAIN=$(basename "$domain_dir")
              
              echo "Processing domain: $DOMAIN"
              
              # Create target directory
              mkdir -p "results/$DOMAIN"
              
              # Copy all files to results
              cp -r "$domain_dir"/* "results/$DOMAIN/" 2>/dev/null || true
              
              # Merge HTTPx results if they exist
              if ls "results/$DOMAIN"/httpx_results-*.txt 1> /dev/null 2>&1; then
                  echo "  -> Merging HTTPx results for $DOMAIN..."
                  
                  # Concatenate all httpx result chunks
                  cat "results/$DOMAIN"/httpx_results-*.txt | sort -u > "results/$DOMAIN/httpx_results.txt"
                  
                  # Remove chunk files
                  rm -f "results/$DOMAIN"/httpx_results-*.txt
                  
                  HTTPX_COUNT=$(wc -l < "results/$DOMAIN/httpx_results.txt" 2>/dev/null || echo 0)
                  echo "  -> ✅ Merged HTTPx results: $HTTPX_COUNT unique URLs"
              fi
          done
          
          echo "✅ All results merged successfully"
      
      - name: Commit and Push All Results
        shell: bash
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add results/
          
          if git diff --staged --quiet; then
              echo "No new results to commit"
          else
              git commit -m "Add scan results (massdns, ports, httpx) [automated]"
              git pull --rebase origin ${{ github.ref_name }}
              git push origin ${{ github.ref_name }}
              echo "✅ All results committed and pushed"
          fi
  
  dispatch-next:
    name: dispatch-next-run
    runs-on: ubuntu-latest
    
    needs: [prepare, brute]
    if: always()
    env:
      PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
    steps:
      - name: Compute and dispatch next run
        shell: bash
        run: |
          if [ -z "${PAT_TOKEN}" ]; then echo "Error: secrets.PAT_TOKEN is required." >&2; exit 0; fi
          CHUNK_START=${{ github.event.inputs.chunk_start }}
          TOTAL_CHUNKS=${{ needs.prepare.outputs.total-chunks }}
          RUN_COUNTER=${{ github.event.inputs.run_counter }}
          MAX_RUNS=${{ needs.prepare.outputs.effective-max-runs }}
          MATRIX_SIZE=${{ env.MATRIX_SIZE }}
          NEXT_START=$((CHUNK_START + MATRIX_SIZE))
          NEXT_COUNTER=$((RUN_COUNTER + 1))
          echo "Current start: ${CHUNK_START}, Next start: ${NEXT_START}, Total chunks: ${TOTAL_CHUNKS}"
          if [ "$NEXT_START" -ge "$TOTAL_CHUNKS" ]; then echo "All chunks processed. Terminating."; exit 0; fi
          if [ "${NEXT_COUNTER}" -ge "${MAX_RUNS}" ]; then echo "Next run would exceed max_runs limit. Terminating."; exit 0; fi
          URL="https://api.github.com/repos/${{ github.repository }}/actions/workflows/${{ env.WORKFLOW_FILE }}/dispatches"
          BODY=$(printf '{"ref":"%s","inputs":{"chunk_start":"%s","total_chunks":"%s","run_counter":"%s","max_runs":"%s","words_per_chunk":"%s","domains_count":"%s"}}' \
            "${{ github.ref_name }}" "${NEXT_START}" "${TOTAL_CHUNKS}" "${NEXT_COUNTER}" "${MAX_RUNS}" \
            "${{ github.event.inputs.words_per_chunk }}" "${{ github.event.inputs.domains_count }}")
          echo "Dispatching next run..."
          for i in 1 2 3 4; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" -X POST -H "Accept: application/vnd.github+json" \
              -H "Authorization: Bearer ${PAT_TOKEN}" -H "X-GitHub-Api-Version: 2022-11-28" -d "$BODY" "$URL")
            if [ "$HTTP_STATUS" -ge 200 ] && [ "$HTTP_STATUS" -lt 300 ]; then echo "Dispatch succeeded (HTTP ${HTTP_STATUS})"; exit 0; fi
            echo "Dispatch attempt ${i} failed (HTTP ${HTTP_STATUS}). Retrying..."
            sleep $((i * 2))
          done
          echo "Failed to dispatch next run after retries. Terminating with an error."
          exit 0
